{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNw+mP0OmgJEMM/fsRjjmoV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Import required libraries"],"metadata":{"id":"2AbId4h3zBm6"}},{"cell_type":"code","source":["import random\n","import numpy as np\n","from collections import defaultdict"],"metadata":{"id":"SnpyuBjNznh5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialize Hyperparameters"],"metadata":{"id":"qoy8fmoQz48f"}},{"cell_type":"code","source":["alpha = 0.1\n","gamma = 0.9\n","epsilon = 0.1\n","epoch= 10000000000"],"metadata":{"id":"-dhe2t9ez5oR","executionInfo":{"status":"ok","timestamp":1758877884040,"user_tz":-330,"elapsed":17,"user":{"displayName":"soul less","userId":"12195366273789735218"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Function to Check Winner in Tic-Tac-Toe"],"metadata":{"id":"F6aC8e_p08Ke"}},{"cell_type":"code","source":["def check_winner(board):\n","    lines = [(0,1,2),(3,4,5),(6,7,8),\n","             (0,3,6),(1,4,7),(2,5,8),\n","             (0,4,8),(2,4,6)]\n","    for i,j,k in lines:\n","        if board[i] == board[j] == board[k] and board[i] != \" \":\n","            return 1 if board[i] == \"x\" else -1\n","    if \" \" not in board:\n","        return 0\n","    return None"],"metadata":{"id":"T2_GqghA086H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialize Q-Table"],"metadata":{"id":"odiNAU432hEr"}},{"cell_type":"code","source":["Q_table = defaultdict(lambda: np.zeros(9))"],"metadata":{"id":"aOIJv9_g2f8E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Epsilon-Greedy Action Selection"],"metadata":{"id":"7k_x5ek11K59"}},{"cell_type":"code","source":["def choose_action(state, available_moves, epsilon=0.1):\n","   # With probability epsilon â†’ explore (random move)\n","    if random.random() < epsilon:\n","        return random.choice(available_moves)\n","    state=tuple(state)\n","    q_vals = Q_table[state]\n","    return max(available_moves, key=lambda a: q_vals[a])"],"metadata":{"id":"e3jkLuz21rc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training Function for Q-Learning Agent in Tic-Tac-Toe"],"metadata":{"id":"ezb2TP5j5QqG"}},{"cell_type":"code","source":["def train(epoch, alpha, gamma, epsilon):\n","    for i in range(epoch):\n","        # Start with an empty board\n","        state = [\" \"] * 9\n","        done = False\n","\n","        while not done:\n","            available_moves = [i for i in range(9) if state[i] == \" \"]\n","            if not available_moves:\n","                break  # No moves left\n","\n","            # Save current state (as tuple for Q-table key)\n","            prev_state = tuple(state)\n","\n","            # Agent (X) move\n","            actionx = choose_action(prev_state, available_moves, epsilon)\n","            state[actionx] = \"x\"\n","\n","            # Check if agentâ€™s move ends the game\n","            reward = check_winner(state)\n","            if reward is not None:\n","                old_val = Q_table[prev_state][actionx]\n","                # Update Q-value using  reward\n","                Q_table[prev_state][actionx] = old_val + alpha * (reward - old_val)\n","                done = True\n","                break\n","\n","            #  Opponent (O) random move\n","            available_opp = [i for i in range(9) if state[i] == \" \"]\n","            opp_move = random.choice(available_opp)\n","            state[opp_move] = \"o\"\n","\n","            # Check result after opponentâ€™s move\n","            reward = check_winner(state)\n","            next_state = tuple(state)\n","\n","            old_val = Q_table[prev_state][actionx]\n","            if reward is not None:\n","                Q_table[prev_state][actionx] = old_val + alpha * (reward - old_val)\n","                done = True\n","            else:\n","                # use Bellman update\n","                next_max = np.max(Q_table[next_state])\n","                Q_table[prev_state][actionx] = old_val + alpha * (0 + gamma * next_max - old_val)"],"metadata":{"id":"xLVJfZ0J5NIh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train the agent"],"metadata":{"id":"j24rkhce6JRl"}},{"cell_type":"code","source":["train(epoch, alpha, gamma, epsilon)"],"metadata":{"id":"A_ZFV5mD56Gm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save Q-Table"],"metadata":{"id":"wWWB7t9dAI_M"}},{"cell_type":"code","source":["import pickle\n","with open(\"q_table.pkl\", \"wb\") as f:\n"," pickle.dump(dict(Q_table), f)"],"metadata":{"id":"WGnl-dDKAIk8","executionInfo":{"status":"ok","timestamp":1758877902353,"user_tz":-330,"elapsed":20,"user":{"displayName":"soul less","userId":"12195366273789735218"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["evaluate"],"metadata":{"id":"y-SefMXk_lug"}},{"cell_type":"code","source":["def agent_move(state, Q_table):\n","    state_t = tuple(state)\n","    q_vals = Q_table[state_t]\n","    available = [i for i in range(9) if state[i] == \" \"]\n","    return max(available, key=lambda a: q_vals[a])\n","\n","# ---- Play one game ----\n","def play_game(use_qtable=False, Q_table=None):\n","    state = [\" \"] * 9\n","\n","    while True:\n","        # X move\n","        available = [i for i in range(9) if state[i] == \" \"]\n","        if not available:\n","            return 0  # draw\n","        if use_qtable:\n","            move = agent_move(state, Q_table)\n","        else:\n","            move = random.choice(available)\n","        state[move] = \"x\"\n","        result = check_winner(state)\n","        if result is not None:\n","            return result\n","\n","        # O random move\n","        available = [i for i in range(9) if state[i] == \" \"]\n","        if not available:\n","            return 0\n","        move = random.choice(available)\n","        state[move] = \"o\"\n","        result = check_winner(state)\n","        if result is not None:\n","            return result\n","\n","# ---- Evaluate accuracy ----\n","def evaluate(n_games=1000, Q_table=None):\n","    def run_eval(use_qtable):\n","        results = {1:0, -1:0, 0:0}\n","        for _ in range(n_games):\n","            res = play_game(use_qtable=use_qtable, Q_table=Q_table)\n","            results[res] += 1\n","        return {k: v/n_games for k,v in results.items()}\n","\n","    rand_stats = run_eval(use_qtable=False)\n","    qtab_stats = run_eval(use_qtable=True)\n","\n","    print(\"ðŸ“Š Evaluation over\", n_games, \"games\")\n","    print(\"\\n-- Random X vs Random O --\")\n","    print(f\"Wins : {rand_stats[1]:.2%}\")\n","    print(f\"Draws: {rand_stats[0]:.2%}\")\n","    print(f\"Loss : {rand_stats[-1]:.2%}\")\n","\n","    print(\"\\n-- Q-table X vs Random O --\")\n","    print(f\"Wins : {qtab_stats[1]:.2%}\")\n","    print(f\"Draws: {qtab_stats[0]:.2%}\")\n","    print(f\"Loss : {qtab_stats[-1]:.2%}\")"],"metadata":{"id":"vhcNPg-CTPs9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate(100000, Q_table=Q_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0pqhbWMr905H","executionInfo":{"status":"ok","timestamp":1758806749577,"user_tz":-330,"elapsed":2737,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"d07dca46-b04b-4133-be63-c470c4bc9ea3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ“Š Evaluation over 100000 games\n","\n","-- Random X vs Random O --\n","Wins : 58.49%\n","Draws: 12.65%\n","Loss : 28.86%\n","\n","-- Q-table X vs Random O --\n","Wins : 99.24%\n","Draws: 0.76%\n","Loss : 0.00%\n"]}]}]}