{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"id":"uegjZXFXxLLm","executionInfo":{"status":"ok","timestamp":1759607398588,"user_tz":-330,"elapsed":38,"user":{"displayName":"soul less","userId":"12195366273789735218"}}},"outputs":[],"source":["import tensorflow as tf\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import math\n","import random\n","import matplotlib.pyplot as plt\n","from collections import defaultdict, deque\n","from tqdm import tqdm\n","import numpy as np\n","from collections import defaultdict\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["def available_black(board):\n","    \"\"\"Return move dictionary for black (-1) pieces\"\"\"\n","    d = defaultdict(lambda: np.zeros(4))\n","    n = 8\n","\n","    for i in range(n):\n","        for j in range(n):\n","            if board[i][j] < 0:  # Black pieces\n","                # Forward-left (up the board)\n","                if i-1 >= 0 and j-1 >= 0:\n","                    if board[i-1][j-1] == 0:\n","                        d[i, j][0] = 1\n","                    elif board[i-1][j-1] > 0 and i-2 >= 0 and j-2 >= 0 and board[i-2][j-2] == 0:\n","                        d[i, j][0] = 1\n","                # Forward-right\n","                if i-1 >= 0 and j+1 < n:\n","                    if board[i-1][j+1] == 0:\n","                        d[i, j][1] = 1\n","                    elif board[i-1][j+1] > 0 and i-2 >= 0 and j+2 < n and board[i-2][j+2] == 0:\n","                        d[i, j][1] = 1\n","                # Backward-left (only for kings)\n","                if board[i][j] == -2 and i+1 < n and j-1 >= 0:\n","                    if board[i+1][j-1] == 0:\n","                        d[i, j][2] = 1\n","                    elif board[i+1][j-1] > 0 and i+2 < n and j-2 >= 0 and board[i+2][j-2] == 0:\n","                        d[i, j][2] = 1\n","                # Backward-right (only for kings)\n","                if board[i][j] == -2 and i+1 < n and j+1 < n:\n","                    if board[i+1][j+1] == 0:\n","                        d[i, j][3] = 1\n","                    elif board[i+1][j+1] > 0 and i+2 < n and j+2 < n and board[i+2][j+2] == 0:\n","                        d[i, j][3] = 1\n","    return d\n","\n","def get_valid_moves_for_black(board):\n","    \"\"\"Return list of valid moves for black (-1)\"\"\"\n","    d = available_black(board)\n","    move_list = []\n","    for pos, arr in d.items():\n","        for i, val in enumerate(arr):\n","            if val == 1:\n","                move_list.append((pos, i))\n","    return move_list\n","\n","def possible_cap_black(board, i, j):\n","    \"\"\"Return possible captures for black piece at (i,j)\"\"\"\n","    d = defaultdict(lambda: np.zeros(4))\n","    n = 8\n","\n","    if board[i][j] < 0:\n","        # Forward-left\n","        if i-2 >= 0 and j-2 >= 0 and board[i-2][j-2] == 0 and board[i-1][j-1] > 0:\n","            d[i, j][0] = 1\n","        # Forward-right\n","        if i-2 >= 0 and j+2 < n and board[i-2][j+2] == 0 and board[i-1][j+1] > 0:\n","            d[i, j][1] = 1\n","        # Backward-left (king)\n","        if board[i][j] == -2 and i+2 < n and j-2 >= 0 and board[i+2][j-2] == 0 and board[i+1][j-1] > 0:\n","            d[i, j][2] = 1\n","        # Backward-right (king)\n","        if board[i][j] == -2 and i+2 < n and j+2 < n and board[i+2][j+2] == 0 and board[i+1][j+1] > 0:\n","            d[i, j][3] = 1\n","    return [((i, j), move) for move, val in enumerate(d[i, j]) if val == 1]\n","\n","def apply_move_black(board, action):\n","    \"\"\"Apply move for black piece (-1)\"\"\"\n","    i, j = action[0]\n","    move = action[1]\n","\n","    piece = board[i][j]\n","    n = 8\n","\n","    if move == 0:  # forward-left\n","        if board[i-1][j-1] == 0:\n","            board[i-1][j-1] = piece\n","            board[i][j] = 0\n","        else:  # capture\n","            board[i-1][j-1] = 0\n","            board[i-2][j-2] = piece\n","            board[i][j] = 0\n","            # Check for multi-capture\n","            for mv in possible_cap_black(board, i-2, j-2):\n","                return apply_move_black(board, mv)\n","    elif move == 1:  # forward-right\n","        if board[i-1][j+1] == 0:\n","            board[i-1][j+1] = piece\n","            board[i][j] = 0\n","        else:\n","            board[i-1][j+1] = 0\n","            board[i-2][j+2] = piece\n","            board[i][j] = 0\n","            for mv in possible_cap_black(board, i-2, j+2):\n","                return apply_move_black(board, mv)\n","    elif move == 2:  # backward-left\n","        board[i+1][j-1] = piece\n","        board[i][j] = 0\n","    elif move == 3:  # backward-right\n","        board[i+1][j+1] = piece\n","        board[i][j] = 0\n","\n","    # King promotion\n","    if i-1 == 0 and piece == -1:\n","        board[i-1][j-1 if move in [0,2] else j+1] = -2\n","\n","    return board\n"],"metadata":{"id":"MByrzTK_us-l","executionInfo":{"status":"ok","timestamp":1759607399540,"user_tz":-330,"elapsed":40,"user":{"displayName":"soul less","userId":"12195366273789735218"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = CheckersCNN().to(device)\n","\n","# --------------------------\n","# Save the model weights\n","# --------------------------\n","torch.save(model.state_dict(), \"/content/checkers_model_2 (3).pth\")\n","\n","# --------------------------\n","# Load the model weights\n","# --------------------------\n","loaded_model = CheckersCNN().to(device)\n","state_dict = torch.load(\"/content/checkers_model_2 (3).pth\", map_location=device)\n","loaded_model.load_state_dict(state_dict)\n","loaded_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"doi4WedAus72","executionInfo":{"status":"ok","timestamp":1759607874105,"user_tz":-330,"elapsed":3624,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"cc72c59e-7fe9-4f91-faad-a8b33078239f"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CheckersCNN(\n","  (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn_in): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (res_blocks): ModuleList(\n","    (0-7): 8 x ResidualBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (policy_conv): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n","  (policy_bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (policy_fc): Linear(in_features=768, out_features=256, bias=True)\n","  (value_conv): Conv2d(512, 6, kernel_size=(1, 1), stride=(1, 1))\n","  (value_bn): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (value_fc1): Linear(in_features=384, out_features=256, bias=True)\n","  (value_fc2): Linear(in_features=256, out_features=64, bias=True)\n","  (value_fc3): Linear(in_features=64, out_features=16, bias=True)\n","  (value_fc4): Linear(in_features=16, out_features=1, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["\n","\n","\n","\n","\n","def play_vs_agent_black(model=model, human_player=-1, num_simulations=50):\n","    game = FixedCheckers()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    model.eval()\n","\n","    while game.EndGame() == 0:\n","        print(game.board)\n","        if game.current_player == human_player:\n","            moves = get_valid_moves_for_black(game.board)\n","            print(\"Your moves:\", moves)\n","            move_idx = int(input(f\"Select move index (0-{len(moves)-1}): \"))\n","            move = moves[move_idx]\n","            apply_move_black(game.board, move)\n","            game.switch_player()\n","        else:\n","            mcts = MCTS(model, device)\n","            policy, _ = mcts.search(game, num_simulations=num_simulations)\n","            valid_moves_dict = game.get_valid_moves()\n","            valid_moves = game.get_moves(valid_moves_dict)\n","            # Pick highest probability move\n","            best_move = max(valid_moves, key=lambda m: policy[m[0][0]*32 + m[0][1]*4 + m[1]])\n","            game.apply_move(best_move)\n","            game.switch_player()\n","\n","    print(\"Game over:\", game.EndGame())\n","    print(game.board)\n","\n"],"metadata":{"id":"1eXLTcdqus41","executionInfo":{"status":"ok","timestamp":1759607874116,"user_tz":-330,"elapsed":3,"user":{"displayName":"soul less","userId":"12195366273789735218"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["\n","\n","play_vs_agent_black()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vkTwoNpXvfLy","outputId":"76499139-79ab-44c6-ee9d-1c5a59c60cf2","executionInfo":{"status":"error","timestamp":1759608601046,"user_tz":-330,"elapsed":726184,"user":{"displayName":"soul less","userId":"12195366273789735218"}}},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  0  0  1  0  1  0  1]\n"," [ 1  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((5, 0), 1), ((5, 2), 0), ((5, 2), 1), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((5, 6), 1)]\n","Select move index (0-6): 2\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  0  0  1  0  1  0  1]\n"," [ 1  0  0  0  0  0  0  0]\n"," [ 0  0  0 -1  0  0  0  0]\n"," [-1  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  1  0  0  0  0  0]\n"," [ 0  0  0 -1  0  0  0  0]\n"," [-1  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((4, 3), 0), ((4, 3), 1), ((5, 0), 1), ((5, 4), 1), ((5, 6), 0), ((5, 6), 1), ((6, 1), 1), ((6, 3), 0)]\n","Select move index (0-7): 0\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0 -1  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 0  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  1  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((5, 0), 1), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((5, 6), 1), ((6, 1), 1), ((6, 3), 0)]\n","Select move index (0-6): 0\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 0  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  1  0  0  0  0  0]\n"," [ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0  0  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  1  0  0  0  0  0]\n"," [ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((4, 1), 1), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((5, 6), 1), ((6, 1), 0), ((6, 1), 1), ((6, 3), 0)]\n","Select move index (0-7): 0\n","[[ 0 -1  0  1  0  1  0  1]\n"," [ 1  0  0  0  1  0  1  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((5, 6), 1), ((6, 1), 0), ((6, 1), 1), ((6, 3), 0)]\n","Select move index (0-6): 0\n","[[ 0 -1  0  0  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  0  0]\n"," [ 0  0  0 -1  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0 -1  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((4, 3), 0), ((4, 3), 1), ((5, 6), 0), ((5, 6), 1), ((6, 1), 0), ((6, 1), 1), ((6, 3), 0), ((6, 3), 1), ((6, 5), 0)]\n","Select move index (0-8): 0\n","[[ 0 -1  0  0  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0 -1  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  1  0  1]\n"," [ 1  0  1  0  0  0  1  0]\n"," [ 0  0  0  1  0  0  0  1]\n"," [ 1  0 -1  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((3, 2), 0), ((3, 2), 1), ((5, 6), 0), ((5, 6), 1), ((6, 1), 0), ((6, 1), 1), ((6, 3), 0), ((6, 3), 1), ((6, 5), 0)]\n","Select move index (0-8): 1\n","[[ 0 -1  0  0  0  1  0  1]\n"," [ 1  0  1  0 -1  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  1  0  1]\n"," [ 1  0  1  0 -1  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((1, 4), 0), ((5, 6), 0), ((5, 6), 1), ((6, 1), 0), ((6, 1), 1), ((6, 3), 0), ((6, 3), 1), ((6, 5), 0)]\n","Select move index (0-7): 0\n","[[ 0 -1  0 -2  0  1  0  1]\n"," [ 1  0  1  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0 -2  0  0  0  1]\n"," [ 1  0  1  0  1  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((0, 3), 2), ((5, 6), 0), ((5, 6), 1), ((6, 1), 0), ((6, 1), 1), ((6, 3), 0), ((6, 3), 1), ((6, 5), 0)]\n","Select move index (0-7): 0\n","[[ 0 -1  0  0  0  0  0  1]\n"," [ 1  0 -2  0  1  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 1  0 -2  0  1  0  1  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((1, 2), 1), ((1, 2), 2), ((1, 2), 3), ((5, 6), 0), ((5, 6), 1), ((6, 1), 0), ((6, 1), 1), ((6, 3), 0), ((6, 3), 1), ((6, 5), 0)]\n","Select move index (0-9): 1\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 1  0  0  0  1  0  1  0]\n"," [ 0 -2  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 1  0  0  0  1  0  1  0]\n"," [ 0 -2  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((2, 1), 1), ((2, 1), 3), ((5, 6), 0), ((6, 1), 0), ((6, 1), 1), ((6, 3), 0), ((6, 3), 1), ((6, 5), 0)]\n","Select move index (0-7): 3\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 1  0  0  0  1  0  1  0]\n"," [ 0 -2  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [-1  0  0  0  0  0 -1  0]\n"," [ 0  0  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 1  0  0  0  1  0  1  0]\n"," [ 0 -2  0  0  0  0  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [-1  0  0  0  0  0 -1  0]\n"," [ 0  0  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((2, 1), 1), ((2, 1), 3), ((5, 0), 1), ((5, 6), 0), ((6, 3), 0), ((6, 3), 1), ((6, 5), 0), ((7, 0), 1), ((7, 2), 0)]\n","Select move index (0-8): 6\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 1  0  0  0  1  0  1  0]\n"," [ 0 -2  0  0  0  0  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [-1  0  0  0 -1  0 -1  0]\n"," [ 0  0  0 -1  0  0  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0  1  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [-1  0  0  0 -1  0 -1  0]\n"," [ 0  0  0 -1  0  0  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((5, 0), 1), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((6, 3), 0), ((7, 0), 1), ((7, 2), 0), ((7, 4), 1), ((7, 6), 0)]\n","Select move index (0-8): 0\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0  1  0  0  0  1  0]\n"," [ 0 -1  0  0  0  0  0  1]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0  0  0 -1  0  0  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0  0  0 -1  0 -1  0]\n"," [ 0  0  0 -1  0  0  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((6, 3), 0), ((7, 0), 1), ((7, 2), 0), ((7, 4), 1), ((7, 6), 0)]\n","Select move index (0-7): 3\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0 -1  0 -1  0 -1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  0  0  1  0  0  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0 -1  0 -1  0 -1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Your moves: [((5, 2), 0), ((5, 2), 1), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((7, 0), 1), ((7, 2), 0), ((7, 2), 1), ((7, 4), 0), ((7, 4), 1), ((7, 6), 0)]\n","Select move index (0-10): 6\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  0  0  1  0  0  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0  0  0  0  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  1  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0  0  0  0  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","Your moves: [((5, 2), 0), ((5, 2), 1), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((7, 4), 0), ((7, 4), 1), ((7, 6), 0)]\n","Select move index (0-7): 0\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  1  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0 -1  0  0  0  0  0  1]\n"," [ 1  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0  0  0  0  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  1  0  0  0  1  0]\n"," [ 0 -1  0  0  0  0  0  1]\n"," [ 1  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0  0  0  0  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","Your moves: [((4, 1), 1), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((6, 1), 1), ((7, 4), 0), ((7, 4), 1), ((7, 6), 0)]\n","Select move index (0-7): 0\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0 -1  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 1  0  0  0 -1  0 -1  0]\n"," [ 0 -1  0  0  0  0  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0 -1  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 1  0  0  0 -1  0  0  0]\n"," [ 0 -1  0  0  0  1  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","Your moves: [((2, 3), 0), ((2, 3), 1), ((5, 4), 0), ((5, 4), 1), ((6, 1), 1), ((6, 7), 0), ((7, 4), 0), ((7, 4), 1)]\n","Select move index (0-7): 0\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -1  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 1  0  0  0 -1  0  0  0]\n"," [ 0 -1  0  0  0  1  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -1  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 1  0  0  0 -1  0  0  0]\n"," [ 0 -1  0  0  0  1  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","Your moves: [((1, 2), 1), ((5, 4), 0), ((5, 4), 1), ((6, 1), 1), ((6, 7), 0), ((7, 4), 0), ((7, 4), 1)]\n","Select move index (0-6): 0\n","[[ 0 -1  0 -2  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 1  0  0  0 -1  0  0  0]\n"," [ 0 -1  0  0  0  1  0 -1]\n"," [-1  0  0  0 -1  0 -1  0]]\n","[[ 0 -1  0 -2  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  1  0 -1]\n"," [-1  0  2  0 -1  0 -1  0]]\n","Your moves: [((0, 3), 2), ((0, 3), 3), ((5, 4), 0), ((5, 4), 1), ((6, 7), 0), ((7, 0), 1), ((7, 4), 0), ((7, 4), 1)]\n","Select move index (0-7): 7\n","[[ 0 -1  0 -2  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  1]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [-1  0  2  0  0  0 -1  0]]\n","[[ 0 -1  0 -2  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [-1  0  2  0  0  0 -1  0]]\n","Your moves: [((0, 3), 2), ((0, 3), 3), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((5, 6), 1), ((7, 0), 1), ((7, 6), 0)]\n","Select move index (0-7): 0\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [-1  0  2  0  0  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0  0  0  2  0  0  0 -1]\n"," [-1  0  0  0  0  0 -1  0]]\n","Your moves: [((1, 2), 1), ((1, 2), 2), ((1, 2), 3), ((5, 4), 0), ((5, 4), 1), ((5, 6), 0), ((5, 6), 1), ((7, 0), 1), ((7, 6), 0)]\n","Select move index (0-8): 3\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  1  0 -1  0  0  0  0]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0  0  0  2  0  0  0 -1]\n"," [-1  0  0  0  0  0 -1  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  1  0  0  0 -1  0]\n"," [ 0  0  0  2  0  0  0 -1]\n"," [-1  0  0  0  0  0 -1  0]]\n","Your moves: [((1, 2), 1), ((1, 2), 2), ((1, 2), 3), ((5, 6), 0), ((5, 6), 1), ((7, 0), 1), ((7, 6), 0)]\n","Select move index (0-6): 6\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  1  0  0  0 -1  0]\n"," [ 0  0  0  2  0 -1  0 -1]\n"," [-1  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  0  0  1]\n"," [ 0  0  1  0  0  0 -1  0]\n"," [ 0  0  0  2  0 -1  0 -1]\n"," [-1  0  0  0  0  0  0  0]]\n","Your moves: [((1, 2), 1), ((1, 2), 2), ((1, 2), 3), ((5, 6), 0), ((6, 5), 0), ((7, 0), 1)]\n","Select move index (0-5): 5\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  0  0  1]\n"," [ 0  0  1  0  0  0 -1  0]\n"," [ 0 -1  0  2  0 -1  0 -1]\n"," [ 0  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0  0  0  2  0 -1  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((1, 2), 1), ((1, 2), 2), ((1, 2), 3), ((5, 6), 0), ((6, 5), 0)]\n","Select move index (0-4): 4\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  0  0  1]\n"," [ 0  0  0  0 -1  0 -1  0]\n"," [ 0  0  0  2  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  2  0  1]\n"," [ 0  0  0  0  0  0 -1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((1, 2), 1), ((1, 2), 2), ((1, 2), 3), ((5, 6), 0)]\n","Select move index (0-3): 3\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  1]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  1  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  1  0]\n"," [ 0  1  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((1, 2), 1), ((1, 2), 2), ((1, 2), 3), ((3, 4), 0), ((3, 4), 1), ((6, 7), 0)]\n","Select move index (0-5): 3\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0 -1  0  0  0  0]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  1  0  0  0  0  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0 -1  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  1  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((1, 2), 1), ((1, 2), 2), ((2, 3), 1), ((6, 7), 0)]\n","Select move index (0-3): 1\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0 -1  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  1  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0 -1  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  1  0  0]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((2, 1), 0), ((2, 1), 1), ((2, 1), 2), ((2, 1), 3), ((2, 3), 0), ((2, 3), 1)]\n","Select move index (0-5): 3\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0 -1  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  1  0  0  0  1  0  0]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0 -1  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  0]\n"," [ 0  0  1  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((2, 3), 0), ((2, 3), 1), ((3, 2), 0), ((3, 2), 2), ((3, 2), 3)]\n","Select move index (0-4): 1\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  0]\n"," [ 0  0  1  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((1, 4), 0), ((1, 4), 1), ((3, 2), 0), ((3, 2), 1), ((3, 2), 2), ((3, 2), 3), ((6, 7), 0)]\n","Select move index (0-6): 4\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0  0  0  0  0  0]\n"," [ 0  0  1  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0  0  0  0  0  0]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  0  0  1  0  0  0 -1]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((1, 4), 0), ((1, 4), 1), ((4, 1), 0), ((4, 1), 1), ((4, 1), 2), ((4, 1), 3), ((6, 7), 0)]\n","Select move index (0-6): 6\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0  0  0 -1  0  0]\n"," [ 0  0  0  0  1  0  0  0]\n"," [ 0  0  0  1  0  0  0  0]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0  0  0 -1  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  1  0  1  0  0]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((1, 4), 0), ((1, 4), 1), ((4, 1), 0), ((4, 1), 1), ((4, 1), 2), ((4, 1), 3), ((4, 5), 0), ((4, 5), 1)]\n","Select move index (0-7): 3\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0 -1  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  1  0  1  0  0]\n"," [ 2  0  0  0  0  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0 -1  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  1  0  0  0  0]\n"," [ 2  0  0  0  2  0  0  0]]\n","Your moves: [((1, 4), 0), ((1, 4), 1), ((3, 2), 0), ((3, 2), 1), ((3, 2), 2), ((3, 2), 3), ((4, 5), 0), ((4, 5), 1)]\n","Select move index (0-7): 4\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0  0  0 -1  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  1  0  0  0  0]\n"," [ 2  0  0  0  2  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0  0  0 -1  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 2  0  2  0  2  0  0  0]]\n","Your moves: [((1, 4), 0), ((1, 4), 1), ((4, 1), 0), ((4, 1), 1), ((4, 1), 2), ((4, 1), 3), ((4, 5), 0), ((4, 5), 1)]\n","Select move index (0-7): 4\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0 -1  0  0]\n"," [-2  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 2  0  2  0  2  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0 -1  0  0]\n"," [-2  0  0  0  0  0  0  0]\n"," [ 0  0  0  2  0  0  0  0]\n"," [ 2  0  0  0  2  0  0  0]]\n","Your moves: [((1, 4), 0), ((1, 4), 1), ((4, 5), 0), ((4, 5), 1), ((5, 0), 1), ((5, 0), 3)]\n","Select move index (0-5): 4\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0  0  0 -1  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  2  0  0  0  0]\n"," [ 2  0  0  0  2  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -2  0  0  0 -1  0  0]\n"," [ 0  0  2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 2  0  0  0  2  0  0  0]]\n","Your moves: [((1, 4), 0), ((1, 4), 1), ((4, 1), 0), ((4, 1), 1), ((4, 1), 2), ((4, 1), 3), ((4, 5), 0), ((4, 5), 1)]\n","Select move index (0-7): 5\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0 -1  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 2  0  0  0  2  0  0  0]]\n","[[ 0 -1  0  0  0  0  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0 -1  0  0]\n"," [ 0  0 -2  0  0  0  0  0]\n"," [ 0  0  0  2  0  0  0  0]\n"," [ 2  0  0  0  0  0  0  0]]\n","Your moves: [((1, 4), 0), ((1, 4), 1), ((4, 5), 0), ((4, 5), 1), ((5, 2), 0), ((5, 2), 1), ((5, 2), 2), ((5, 2), 3)]\n","Select move index (0-7): y\n"]},{"output_type":"error","ename":"ValueError","evalue":"invalid literal for int() with base 10: 'y'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2107167346.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_vs_agent_black\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-4263713811.py\u001b[0m in \u001b[0;36mplay_vs_agent_black\u001b[0;34m(model, human_player, num_simulations)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmoves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_valid_moves_for_black\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your moves:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mmove_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Select move index (0-{len(moves)-1}): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mapply_move_black\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'y'"]}]},{"cell_type":"code","source":["class FixedCheckers:\n","    \"\"\"Fixed version of your Checkers class\"\"\"\n","    def __init__(self):\n","        self.board = np.array([\n","            [0, 1, 0,   1, 0,  1, 0,  1],\n","            [ 1,  0,  1,  0,  1,  0, 1,  0],\n","            [0,  1, 0,  1, 0,  1, 0,   1],\n","            [0,  0, 0,  0, 0,  0, 0,  0],\n","            [0,  0, 0,  0, 0,  0, 0,  0],\n","            [-1, 0,  -1, 0,  -1, 0,  -1, 0],\n","            [0,  -1, 0,  -1, 0,  -1, 0,  -1],\n","            [ -1, 0,  -1, 0, -1, 0,  -1, 0]\n","        ], dtype=int)\n","        self.current_player = 1\n","    def switch_player(self):\n","        \"\"\"Explicitly switch the current player\"\"\"\n","        self.current_player *= -1\n","    def reverse(self):\n","\n","      flipped = [row[::-1] for row in self.board[::-1]]\n","\n","      mapping = {1: -1, 0: 0, 2: -2, -1: 1,0:0  }\n","      self.board= np.array([[mapping.get(cell, cell) for cell in row] for row in flipped])\n","\n","      return self.board\n","\n","    def board_to_tensor(self):\n","\n","            return board_to_tensor(self.board )\n","\n","\n","    def copy(self):\n","        \"\"\"Create a copy of the game state\"\"\"\n","        new_game = FixedCheckers()\n","        new_game.board = self.board.copy()\n","        new_game.current_player = self.current_player\n","        return new_game\n","\n","    def get_valid_moves(self, player=1):\n","        \"\"\"Get valid moves for current player\"\"\"\n","        if player is None:\n","            player = self.current_player\n","        return self.available(player)\n","\n","    def available(self, player=1):\n","        d = defaultdict(lambda: np.zeros(4))\n","\n","\n","\n","        if player == 1:  # white pieces\n","            for i in range(len(self.board)):\n","                for j in range(len(self.board)):\n","                    if self.board[i][j] != 0 and self.board[i][j] >= player:\n","                        # Forward left\n","                        if i+1 < 8 and j-1 >= 0:\n","                            if self.board[i+1][j-1] == 0:\n","                                d[i, j][0] = 1\n","                            elif self.board[i+1][j-1] <  0:\n","                                if i+2 < 8 and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                                    d[i, j][0] = 1\n","\n","                        if i+1 < 8 and j+1 < 8:\n","                            if self.board[i+1][j+1] == 0:\n","                                d[i, j][1] = 1\n","                            elif self.board[i+1][j+1] <  0:\n","                                if i+2 < 8 and j+2 <= 7 and self.board[i+2][j+2] == 0:\n","                                    d[i,j][1] = 1\n","\n","                    if self.board[i][j] != 0 and self.board[i][j] > player:\n","                        # Backward left\n","                        if i-1 >= 0 and j-1 >= 0:\n","                            if self.board[i-1][j-1] == 0:\n","                                d[i, j][2] = 1\n","                            elif self.board[i-1][j-1]  < 0:\n","                                if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                                    d[i, j][2] = 1\n","                        # Backward right\n","                        if i-1 >= 0 and j+1 < 8:\n","                            if self.board[i-1][j+1] == 0:\n","                                d[i, j][3] = 1\n","                            elif self.board[i-1][j+1]  < 0:\n","                                if i-2 >= 0 and j+2 < 8 and self.board[i-2][j+2] == 0:\n","                                    d[i, j][3] = 1\n","        return d\n","\n","\n","    def apply_move(self, action, player=1 ):\n","\n","      i, j = action[0]\n","      move = action[1]\n","\n","      if player == 1:  # White player\n","        if move == 0:  # forward-left\n","            if self.board[i+1][j-1] == 0:\n","                self.board[i+1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j-1] == 1:\n","                    self.board[i+1][j-1] += 1\n","\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j-1]\n","\n","                self.board[i+1][j-1] = 0\n","                self.board[i+2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j-2] == 1:\n","                    self.board[i+2][j-2] += 1\n","\n","                act = self.Possible_cap(i+2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 1:  # forward-right\n","            if self.board[i+1][j+1] == 0:\n","                self.board[i+1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j+1] == 1:\n","                    self.board[i+1][j+1] += 1\n","                    # Extra point for king promotion\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j+1]\n","                   # Remove from black's score\n","\n","                self.board[i+1][j+1] = 0\n","                self.board[i+2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j+2] == 1:\n","                    self.board[i+2][j+2] += 1\n","\n","                act = self.Possible_cap(i+2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 2:  # backward-left\n","            if self.board[i-1][j-1] == 0:\n","                self.board[i-1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j-1]\n","\n","\n","                self.board[i-1][j-1] = 0\n","                self.board[i-2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 3:  # backward-right\n","            if self.board[i-1][j+1] == 0:\n","                self.board[i-1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j+1]\n","\n","\n","                self.board[i-1][j+1] = 0\n","                self.board[i-2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","      return self.board\n","\n","    def Possible_cap(self, i, j, player=1):\n","     d   = defaultdict(lambda: np.zeros(4))\n","     n = 8\n","\n","     if self.board[i][j] >= 1:\n","            # Forward left\n","            if i+2 < n and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                if self.board[i+1][j-1] != 0 and self.board[i+1][j-1] <= 0:\n","                    d[i, j][0] = 1\n","            # Forward right\n","            if i+2 < n and j+2 < n and self.board[i+2][j+2] == 0:\n","                if self.board[i+1][j+1] != 0 and self.board[i+1][j+1] <= 0:\n","                    d[i, j][1] = 1\n","     if self.board[i][j] > 1:\n","\n","            if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                if self.board[i-1][j-1] != 0 and self.board[i-1][j-1] <= 0:\n","                    d[i, j][2] = 1\n","            # Backward right\n","            if i-2 >= 0 and j+2 < n and self.board[i-2][j+2] == 0:\n","                if self.board[i-1][j+1] != 0 and self.board[i-1][j+1] <= 0:\n","                    d[i, j][3] = 1\n","\n","     return self.get_moves(d)\n","\n","\n","\n","    def get_moves(self, d):\n","        \"\"\"Convert move dict to list\"\"\"\n","        move_list = []\n","        for pos, arr in d.items():\n","            for i, val in enumerate(arr):\n","                if val == 1:\n","                    move_list.append((pos, i))\n","        return move_list\n","\n","\n","    def EndGame(self):\n","        \"\"\"Check if game has ended\"\"\"\n","        white_pieces = np.sum(self.board > 0)\n","        black_pieces = np.sum(self.board < 0)\n","\n","        if white_pieces == 0:\n","            return -1 # Black wins\n","        elif black_pieces == 0:\n","            return 1   # White wins\n","\n","        # Check if current player has moves\n","        moves = self.get_moves(self.get_valid_moves( ))\n","        if not moves:\n","            return - 1  # Current player loses\n","\n","        return 0  # Game continues\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out = F.relu(out + residual)  # Skip connection + activation\n","        return out\n","class  CheckersCNN(nn.Module):\n","    def __init__(self, num_res_blocks=8, board_channels=4, torso_channels=512):\n","        super().__init__()\n","\n","        self.conv_in = nn.Conv2d(board_channels, torso_channels, kernel_size=3, padding=1)\n","        self.bn_in = nn.BatchNorm2d(torso_channels)\n","\n","        self.res_blocks = nn.ModuleList([\n","            ResidualBlock(torso_channels) for _ in range(num_res_blocks)\n","        ])\n","\n","        # OPTIMIZED: Policy head (reduced from 1024 to 512 actions)\n","        self.policy_conv = nn.Conv2d(torso_channels, 12, kernel_size=1)  # 16→12 channels\n","        self.policy_bn = nn.BatchNorm2d(12)\n","        self.policy_fc = nn.Linear(12 * 8 * 8, 256)  # 1024→512 actions (still plenty)\n","\n","        # OPTIMIZED: Slightly smaller value head (still deep)\n","        self.value_conv = nn.Conv2d(torso_channels, 6, kernel_size=1)  # 8→6 channels\n","        self.value_bn = nn.BatchNorm2d(6)\n","        self.value_fc1 = nn.Linear(6 * 8 * 8, 256)\n","        self.value_fc2 = nn.Linear(256, 64)  # 128→64 (smaller bottleneck)\n","        self.value_fc3 = nn.Linear(64, 16)   # 32→16 (smaller final layer)\n","        self.value_fc4 = nn.Linear(16, 1)\n","\n","        # ADD: Dropout for regularization (optional)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        # Feature extraction\n","        x = F.relu(self.bn_in(self.conv_in(x)))\n","\n","        for res_block in self.res_blocks:\n","            x = res_block(x)\n","\n","        # Policy head\n","        policy = F.relu(self.policy_bn(self.policy_conv(x)))\n","        policy = policy.view(policy.size(0), -1)\n","        policy = self.dropout(policy)  # Optional: add dropout\n","        policy = self.policy_fc(policy)\n","\n","\n","        # Value head\n","        value = F.relu(self.value_bn(self.value_conv(x)))\n","        value = value.view(value.size(0), -1)\n","        value = F.relu(self.value_fc1(value))\n","        value = self.dropout(value)  # Optional: add dropout\n","        value = F.relu(self.value_fc2(value))\n","        value = F.relu(self.value_fc3(value))\n","        value = torch.tanh(self.value_fc4(value))\n","\n","        return policy, value\n","\n","class Node:\n","    def __init__(self, game_state, parent=None, action_taken=None, prior=0):\n","        self.game_state = game_state.copy()\n","        self.parent = parent\n","        self.action_taken = action_taken\n","        self.prior = prior\n","\n","        self.children = {}\n","        self.visit_count = 0\n","        self.value_sum = 0\n","        self.is_expanded = False\n","\n","    def is_terminal(self):\n","        return self.game_state.EndGame() != 0\n","\n","    def expand(self, policy_logits):\n","\n","     if self.is_expanded:\n","        return\n","\n","    # 1. Get valid moves for current game state\n","     valid_moves_dict = self.game_state.get_valid_moves()\n","     valid_moves = self.game_state.get_moves(valid_moves_dict)\n","     if not valid_moves:\n","        self.is_expanded = True\n","        return\n","\n","    # 2. Create mask for valid moves\n","     mask = torch.zeros_like(policy_logits)\n","     move_map = {}  # Maps index in policy to actual move\n","     for move in valid_moves:\n","        (i, j), direction = move\n","        idx = i * 32 + j * 4 + direction\n","        mask[idx] = 1\n","        move_map[idx ] = move  # store mapping for later\n","\n","    # 3. Apply softmax on logits\n","     if policy_logits.dim() == 1:  # single board\n","            policy_probs = F.softmax(policy_logits, dim=0)\n","     else:  # batch (should not happen in MCTS, but safe)\n","            policy_probs = F.softmax(policy_logits, dim=1)[0]\n","\n","    # 4. Mask invalid moves and renormalize\n","     masked_probs = policy_probs * mask\n","     if masked_probs.sum() > 0:\n","        masked_probs /= masked_probs.sum()\n","     else:\n","        # Fallback: all moves are invalid (should not happen)\n","        masked_probs = mask / mask.sum()\n","\n","    # 5. Create child nodes for all valid moves\n","     for idx, prob in enumerate(masked_probs):\n","        if prob > 0:\n","            move = move_map[idx]\n","            new_game = self.game_state.copy()\n","            new_game.apply_move(move)\n","            new_game.reverse()  # Switch player perspective\n","            child = Node(new_game, parent=self, action_taken=move, prior=prob.item())\n","            self.children[move] = child\n","\n","     self.is_expanded = True\n","\n","    def select_child(self, c_puct=1.0):\n","        \"\"\"Select child with highest UCB score\"\"\"\n","        best_score = -float('inf')\n","        best_child = None\n","\n","        for child in self.children.values():\n","            q_value = 0 if child.visit_count == 0 else child.value_sum / child.visit_count\n","            u_value = c_puct * child.prior * math.sqrt(self.visit_count) / (1 + child.visit_count)\n","            score = q_value + u_value\n","\n","            if score > best_score:\n","                best_score = score\n","                best_child = child\n","\n","        return best_child\n","\n","    def backup(self, value):\n","        \"\"\"Backup value through the tree\"\"\"\n","        self.visit_count += 1\n","        self.value_sum += value\n","\n","        if self.parent:\n","            self.parent.backup(-value)\n","class MCTS:\n","    def __init__(self, model, device, c_puct=1.0):\n","        self.model = model\n","        self.device = device\n","        self.c_puct = c_puct\n","\n","    def search(self, game_state, num_simulations=100):\n","        root = Node(game_state)\n","\n","        for _ in range(num_simulations):\n","            node = root\n","\n","            # -------- Selection --------\n","            while node.is_expanded and not node.is_terminal():\n","                node = node.select_child(self.c_puct)\n","\n","            # -------- Evaluation --------\n","            if node.is_terminal():\n","                value = node.game_state.EndGame()\n","            else:\n","                # Neural network prediction\n","                board_tensor = torch.tensor(\n","                    node.game_state.board_to_tensor(), dtype=torch.float32\n","                ).unsqueeze(0).to(self.device)  # shape: (1, C, 8, 8)\n","\n","                with torch.no_grad():\n","                    policy_logits, value_tensor = self.model(board_tensor)\n","                    policy_logits = policy_logits.squeeze(0)  # (256,)\n","                    value = value_tensor.item()\n","\n","                # -------- Expansion --------\n","                node.expand(policy_logits)\n","\n","            # -------- Backup --------\n","            node.backup(value)\n","\n","        # -------- Build policy from visit counts --------\n","        valid_moves_dict = game_state.get_valid_moves()\n","        valid_moves = game_state.get_moves(valid_moves_dict)\n","        policy = np.zeros(256)\n","\n","        for move in valid_moves:\n","            if move in root.children:\n","                (i, j), direction = move\n","                idx = i * 32 + j * 4 + direction\n","                policy[idx] = root.children[move].visit_count\n","\n","        if policy.sum() > 0:\n","            policy /= policy.sum()\n","\n","\n","        return policy, root.value_sum / max(root.visit_count, 1)\n","\n","\n","def board_to_tensor(board):\n","    \"\"\"Convert board to tensor with 4 planes\"\"\"\n","    planes = np.zeros((4, 8, 8), dtype=np.float32)\n","    planes[0] = (board == 1).astype(np.float32)   # White pieces\n","    planes[1] = (board == 2).astype(np.float32)   # White kings\n","    planes[2] = (board == -1).astype(np.float32)  # Black pieces\n","    planes[3] = (board == -2).astype(np.float32)  # Black kings\n","    return planes\n","\n","class AlphaZeroTrainer:\n","    def __init__(self, model, lr=0.001):\n","        self.model = model\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.model.to(self.device)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n","\n","        # Replay buffer\n","        self.training_data = deque(maxlen=75000)\n","\n","    # -------- Self-play -------- #\n","    def self_play_game(self, num_simulations=100):\n","     game = FixedCheckers()\n","     mcts = MCTS(self.model, self.device)  # create once\n","\n","     states, mcts_policies, players = [], [], []\n","     count = 0\n","\n","     while count <= 200:\n","        count += 1\n","        # Temperature schedule: exploration for first 30 moves\n","        temperature = 1.0 if count <= 6   else 0.0\n","\n","        # Run MCTS (wrap game in list)\n","        policies = mcts.search([game], num_simulations=num_simulations)\n","        policy = policies[0]\n","\n","        # Get valid moves\n","        valid_moves_dict = game.get_valid_moves()\n","        valid_moves = game.get_moves(valid_moves_dict)\n","        if not valid_moves:\n","            break\n","\n","        # Extract policy probabilities for valid moves\n","        valid_policy = []\n","        for move in valid_moves:\n","            (i, j), direction = move\n","            idx = i * 32 + j * 4 + direction\n","            valid_policy.append(policy[idx])\n","        valid_policy = np.array(valid_policy)\n","        valid_policy = np.nan_to_num(valid_policy)  # replace any NaNs with 0\n","\n","        # Fallback if all probabilities are zero\n","        if valid_policy.sum() == 0:\n","            valid_policy = np.ones_like(valid_policy) / len(valid_policy)\n","\n","        # Choose move using temperature\n","        if temperature == 0:\n","            move_idx = np.argmax(valid_policy)\n","        else:\n","            valid_policy = valid_policy ** (1 / temperature)\n","            valid_policy /= valid_policy.sum()\n","            move_idx = np.random.choice(len(valid_policy), p=valid_policy)\n","\n","        move = valid_moves[move_idx]\n","\n","        # Record training data\n","        player_to_move = game.current_player\n","        board_tensor = board_to_tensor(game.board)\n","        states.append(board_tensor)\n","        mcts_policies.append(policy)\n","        players.append(player_to_move)\n","\n","        # Apply move\n","        game.apply_move(move)\n","        game.reverse()\n","        game.switch_player()\n","\n","    # Assign outcome\n","     result = game.EndGame()\n","     data = []\n","     for state, policy, player in zip(states, mcts_policies, players):\n","        value = 0 if result == 0 else (result if player == 1 else -result)\n","        data.append((state, policy, value))\n","\n","     return data\n","\n","    def train_step(self, batch_size=2048):\n","     if len(self.training_data) < batch_size:\n","        return 0, 0\n","\n","     batch = random.sample(list(self.training_data), batch_size)\n","\n","    # Efficient stacking of NumPy arrays\n","     boards = torch.tensor(np.stack([x[0] for x in batch]), dtype=torch.float32).to(self.device)\n","     target_policies = torch.tensor(np.stack([x[1] for x in batch]), dtype=torch.float32).to(self.device)\n","     target_values = torch.tensor([[x[2]] for x in batch], dtype=torch.float32).to(self.device)\n","\n","    # Forward pass\n","     pred_logits, pred_values = self.model(boards)\n","\n","    # Policy loss (cross-entropy with MCTS distribution)\n","     pred_log_probs = F.log_softmax(pred_logits, dim=1)\n","     policy_loss = -(target_policies * pred_log_probs).sum(dim=1).mean()\n","\n","    # Value loss (MSE)\n","     value_loss = F.mse_loss(pred_values, target_values)\n","\n","     total_loss = policy_loss + value_loss\n","\n","    # Backward pass\n","     self.optimizer.zero_grad()\n","     total_loss.backward()\n","     torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","     self.optimizer.step()\n","\n","     return policy_loss.item(), value_loss.item()\n","\n","\n","    # -------- Main training loop -------- #\n","    def train(self, iterations=1000, games_per_iteration=10, train_steps_per_iteration=10):\n","        print(f\"Training on device: {self.device}\")\n","\n","        policy_losses = []\n","        value_losses = []\n","\n","        for iteration in tqdm(range(iterations)):\n","            # -------- Self-play --------\n","            for _ in range(games_per_iteration):\n","                # Anneal temperature: exploration in first 30 moves, greedy later\n","                examples = self.self_play_game(\n","                                         num_simulations=200,\n","                                          )\n","\n","                self.training_data.extend(examples)\n","\n","            # -------- Training --------\n","            total_policy_loss = 0\n","            total_value_loss = 0\n","\n","            for _ in range(train_steps_per_iteration):\n","                p_loss, v_loss = self.train_step()\n","                total_policy_loss += p_loss\n","                total_value_loss += v_loss\n","\n","            avg_policy_loss = total_policy_loss / max(train_steps_per_iteration, 1)\n","            avg_value_loss = total_value_loss / max(train_steps_per_iteration, 1)\n","\n","            policy_losses.append(avg_policy_loss)\n","            value_losses.append(avg_value_loss)\n","\n","            if iteration % 1 == 0:\n","                print(f\"Iteration {iteration}: Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}\")\n","                print(f\"Training data size: {len(self.training_data)}\")\n","\n","                # Save checkpoint\n","                torch.save(self.model.state_dict(), f'checkers_model_{iteration}.pth')\n","\n","        # -------- Plot training curves --------\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","        ax1.plot(policy_losses)\n","        ax1.set_title('Policy Loss')\n","        ax1.set_xlabel('Iteration')\n","        ax1.set_ylabel('Loss')\n","\n","        ax2.plot(value_losses)\n","        ax2.set_title('Value Loss')\n","        ax2.set_xlabel('Iteration')\n","        ax2.set_ylabel('Loss')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Save final model\n","        torch.save(self.model.state_dict(), 'checkers_final_model.pth')\n","\n","        return policy_losses, value_losses"],"metadata":{"id":"XwTZavE8VRwi","executionInfo":{"status":"ok","timestamp":1759607405244,"user_tz":-330,"elapsed":154,"user":{"displayName":"soul less","userId":"12195366273789735218"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"66CVkxvVusz8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JzW7CfeXusxZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OPoWcYOJusur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FixedCheckers:\n","    \"\"\"Fixed version of your Checkers class\"\"\"\n","    def __init__(self):\n","        self.board = np.array([\n","            [0, 1, 0,   1, 0,  1, 0,  1],\n","            [ 1,  0,  1,  0,  1,  0, 1,  0],\n","            [0,  1, 0,  1, 0,  1, 0,   1],\n","            [0,  0, 0,  0, 0,  0, 0,  0],\n","            [0,  0, 0,  0, 0,  0, 0,  0],\n","            [-1, 0,  -1, 0,  -1, 0,  -1, 0],\n","            [0,  -1, 0,  -1, 0,  -1, 0,  -1],\n","            [ -1, 0,  -1, 0, -1, 0,  -1, 0]\n","        ], dtype=int)\n","        self.current_player = 1\n","    def switch_player(self):\n","        \"\"\"Explicitly switch the current player\"\"\"\n","        self.current_player *= -1\n","    def reverse(self):\n","\n","      flipped = [row[::-1] for row in self.board[::-1]]\n","\n","      mapping = {1: -1, 0: 0, 2: -2, -1: 1,0:0  }\n","      self.board= np.array([[mapping.get(cell, cell) for cell in row] for row in flipped])\n","\n","      return self.board\n","\n","    def board_to_tensor(self):\n","\n","            return board_to_tensor(self.board )\n","\n","\n","    def copy(self):\n","        \"\"\"Create a copy of the game state\"\"\"\n","        new_game = FixedCheckers()\n","        new_game.board = self.board.copy()\n","        new_game.current_player = self.current_player\n","        return new_game\n","\n","    def get_valid_moves(self, player=1):\n","        \"\"\"Get valid moves for current player\"\"\"\n","        if player is None:\n","            player = self.current_player\n","        return self.available(player)\n","\n","    def available(self, player=1):\n","        d = defaultdict(lambda: np.zeros(4))\n","\n","\n","\n","        if player == 1:  # white pieces\n","            for i in range(len(self.board)):\n","                for j in range(len(self.board)):\n","                    if self.board[i][j] != 0 and self.board[i][j] >= player:\n","                        # Forward left\n","                        if i+1 < 8 and j-1 >= 0:\n","                            if self.board[i+1][j-1] == 0:\n","                                d[i, j][0] = 1\n","                            elif self.board[i+1][j-1] <  0:\n","                                if i+2 < 8 and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                                    d[i, j][0] = 1\n","\n","                        if i+1 < 8 and j+1 < 8:\n","                            if self.board[i+1][j+1] == 0:\n","                                d[i, j][1] = 1\n","                            elif self.board[i+1][j+1] <  0:\n","                                if i+2 < 8 and j+2 <= 7 and self.board[i+2][j+2] == 0:\n","                                    d[i,j][1] = 1\n","\n","                    if self.board[i][j] != 0 and self.board[i][j] > player:\n","                        # Backward left\n","                        if i-1 >= 0 and j-1 >= 0:\n","                            if self.board[i-1][j-1] == 0:\n","                                d[i, j][2] = 1\n","                            elif self.board[i-1][j-1]  < 0:\n","                                if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                                    d[i, j][2] = 1\n","                        # Backward right\n","                        if i-1 >= 0 and j+1 < 8:\n","                            if self.board[i-1][j+1] == 0:\n","                                d[i, j][3] = 1\n","                            elif self.board[i-1][j+1]  < 0:\n","                                if i-2 >= 0 and j+2 < 8 and self.board[i-2][j+2] == 0:\n","                                    d[i, j][3] = 1\n","        return d\n","\n","\n","    def apply_move(self, action, player=1 ):\n","\n","      i, j = action[0]\n","      move = action[1]\n","\n","      if player == 1:  # White player\n","        if move == 0:  # forward-left\n","            if self.board[i+1][j-1] == 0:\n","                self.board[i+1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j-1] == 1:\n","                    self.board[i+1][j-1] += 1\n","\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j-1]\n","\n","                self.board[i+1][j-1] = 0\n","                self.board[i+2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j-2] == 1:\n","                    self.board[i+2][j-2] += 1\n","\n","                act = self.Possible_cap(i+2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 1:  # forward-right\n","            if self.board[i+1][j+1] == 0:\n","                self.board[i+1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j+1] == 1:\n","                    self.board[i+1][j+1] += 1\n","                    # Extra point for king promotion\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j+1]\n","                   # Remove from black's score\n","\n","                self.board[i+1][j+1] = 0\n","                self.board[i+2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j+2] == 1:\n","                    self.board[i+2][j+2] += 1\n","\n","                act = self.Possible_cap(i+2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 2:  # backward-left\n","            if self.board[i-1][j-1] == 0:\n","                self.board[i-1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j-1]\n","\n","\n","                self.board[i-1][j-1] = 0\n","                self.board[i-2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 3:  # backward-right\n","            if self.board[i-1][j+1] == 0:\n","                self.board[i-1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j+1]\n","\n","\n","                self.board[i-1][j+1] = 0\n","                self.board[i-2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","      return self.board\n","\n","    def Possible_cap(self, i, j, player=1):\n","     d   = defaultdict(lambda: np.zeros(4))\n","     n = 8\n","\n","     if self.board[i][j] >= 1:\n","            # Forward left\n","            if i+2 < n and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                if self.board[i+1][j-1] != 0 and self.board[i+1][j-1] <= 0:\n","                    d[i, j][0] = 1\n","            # Forward right\n","            if i+2 < n and j+2 < n and self.board[i+2][j+2] == 0:\n","                if self.board[i+1][j+1] != 0 and self.board[i+1][j+1] <= 0:\n","                    d[i, j][1] = 1\n","     if self.board[i][j] > 1:\n","\n","            if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                if self.board[i-1][j-1] != 0 and self.board[i-1][j-1] <= 0:\n","                    d[i, j][2] = 1\n","            # Backward right\n","            if i-2 >= 0 and j+2 < n and self.board[i-2][j+2] == 0:\n","                if self.board[i-1][j+1] != 0 and self.board[i-1][j+1] <= 0:\n","                    d[i, j][3] = 1\n","\n","     return self.get_moves(d)\n","\n","\n","\n","    def get_moves(self, d):\n","        \"\"\"Convert move dict to list\"\"\"\n","        move_list = []\n","        for pos, arr in d.items():\n","            for i, val in enumerate(arr):\n","                if val == 1:\n","                    move_list.append((pos, i))\n","        return move_list\n","\n","\n","    def EndGame(self):\n","        \"\"\"Check if game has ended\"\"\"\n","        white_pieces = np.sum(self.board > 0)\n","        black_pieces = np.sum(self.board < 0)\n","\n","        if white_pieces == 0:\n","            return -1 # Black wins\n","        elif black_pieces == 0:\n","            return 1   # White wins\n","\n","        # Check if current player has moves\n","        moves = self.get_moves(self.get_valid_moves( ))\n","        if not moves:\n","            return - 1  # Current player loses\n","\n","        return 0  # Game continues\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out = F.relu(out + residual)  # Skip connection + activation\n","        return out\n","class  CheckersCNN(nn.Module):\n","    def __init__(self, num_res_blocks=8, board_channels=4, torso_channels=512):\n","        super().__init__()\n","\n","        self.conv_in = nn.Conv2d(board_channels, torso_channels, kernel_size=3, padding=1)\n","        self.bn_in = nn.BatchNorm2d(torso_channels)\n","\n","        self.res_blocks = nn.ModuleList([\n","            ResidualBlock(torso_channels) for _ in range(num_res_blocks)\n","        ])\n","\n","        # OPTIMIZED: Policy head (reduced from 1024 to 512 actions)\n","        self.policy_conv = nn.Conv2d(torso_channels, 12, kernel_size=1)  # 16→12 channels\n","        self.policy_bn = nn.BatchNorm2d(12)\n","        self.policy_fc = nn.Linear(12 * 8 * 8, 256)  # 1024→512 actions (still plenty)\n","\n","        # OPTIMIZED: Slightly smaller value head (still deep)\n","        self.value_conv = nn.Conv2d(torso_channels, 6, kernel_size=1)  # 8→6 channels\n","        self.value_bn = nn.BatchNorm2d(6)\n","        self.value_fc1 = nn.Linear(6 * 8 * 8, 256)\n","        self.value_fc2 = nn.Linear(256, 64)  # 128→64 (smaller bottleneck)\n","        self.value_fc3 = nn.Linear(64, 16)   # 32→16 (smaller final layer)\n","        self.value_fc4 = nn.Linear(16, 1)\n","\n","        # ADD: Dropout for regularization (optional)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        # Feature extraction\n","        x = F.relu(self.bn_in(self.conv_in(x)))\n","\n","        for res_block in self.res_blocks:\n","            x = res_block(x)\n","\n","        # Policy head\n","        policy = F.relu(self.policy_bn(self.policy_conv(x)))\n","        policy = policy.view(policy.size(0), -1)\n","        policy = self.dropout(policy)  # Optional: add dropout\n","        policy = self.policy_fc(policy)\n","\n","\n","        # Value head\n","        value = F.relu(self.value_bn(self.value_conv(x)))\n","        value = value.view(value.size(0), -1)\n","        value = F.relu(self.value_fc1(value))\n","        value = self.dropout(value)  # Optional: add dropout\n","        value = F.relu(self.value_fc2(value))\n","        value = F.relu(self.value_fc3(value))\n","        value = torch.tanh(self.value_fc4(value))\n","\n","        return policy, value\n","\n","class Node:\n","    def __init__(self, game_state, parent=None, action_taken=None, prior=0):\n","        self.game_state = game_state.copy()\n","        self.parent = parent\n","        self.action_taken = action_taken\n","        self.prior = prior\n","\n","        self.children = {}\n","        self.visit_count = 0\n","        self.value_sum = 0\n","        self.is_expanded = False\n","\n","    def is_terminal(self):\n","        return self.game_state.EndGame() != 0\n","\n","    def expand(self, policy_logits):\n","\n","     if self.is_expanded:\n","        return\n","\n","    # 1. Get valid moves for current game state\n","     valid_moves_dict = self.game_state.get_valid_moves()\n","     valid_moves = self.game_state.get_moves(valid_moves_dict)\n","     if not valid_moves:\n","        self.is_expanded = True\n","        return\n","\n","    # 2. Create mask for valid moves\n","     mask = torch.zeros_like(policy_logits)\n","     move_map = {}  # Maps index in policy to actual move\n","     for move in valid_moves:\n","        (i, j), direction = move\n","        idx = i * 32 + j * 4 + direction\n","        mask[idx] = 1\n","        move_map[idx ] = move  # store mapping for later\n","\n","    # 3. Apply softmax on logits\n","     if policy_logits.dim() == 1:  # single board\n","            policy_probs = F.softmax(policy_logits, dim=0)\n","     else:  # batch (should not happen in MCTS, but safe)\n","            policy_probs = F.softmax(policy_logits, dim=1)[0]\n","\n","    # 4. Mask invalid moves and renormalize\n","     masked_probs = policy_probs * mask\n","     if masked_probs.sum() > 0:\n","        masked_probs /= masked_probs.sum()\n","     else:\n","        # Fallback: all moves are invalid (should not happen)\n","        masked_probs = mask / mask.sum()\n","\n","    # 5. Create child nodes for all valid moves\n","     for idx, prob in enumerate(masked_probs):\n","        if prob > 0:\n","            move = move_map[idx]\n","            new_game = self.game_state.copy()\n","            new_game.apply_move(move)\n","            new_game.reverse()  # Switch player perspective\n","            child = Node(new_game, parent=self, action_taken=move, prior=prob.item())\n","            self.children[move] = child\n","\n","     self.is_expanded = True\n","\n","    def select_child(self, c_puct=1.0):\n","        \"\"\"Select child with highest UCB score\"\"\"\n","        best_score = -float('inf')\n","        best_child = None\n","\n","        for child in self.children.values():\n","            q_value = 0 if child.visit_count == 0 else child.value_sum / child.visit_count\n","            u_value = c_puct * child.prior * math.sqrt(self.visit_count) / (1 + child.visit_count)\n","            score = q_value + u_value\n","\n","            if score > best_score:\n","                best_score = score\n","                best_child = child\n","\n","        return best_child\n","\n","    def backup(self, value):\n","        \"\"\"Backup value through the tree\"\"\"\n","        self.visit_count += 1\n","        self.value_sum += value\n","\n","        if self.parent:\n","            self.parent.backup(-value)\n","class MCTS:\n","    def __init__(self, model, device, c_puct=1.0):\n","        self.model = model\n","        self.device = device\n","        self.c_puct = c_puct\n","\n","    @torch.no_grad()\n","    def search(self, game_states, num_simulations=50):\n","        \"\"\"\n","        game_states: list of FixedCheckers objects\n","        Returns: list of policy arrays (256,) for each state\n","        \"\"\"\n","        roots = [Node(state) for state in game_states]\n","        for _ in range(num_simulations):  # FIXED: was for * in range(num*simulations)\n","            # ---------- Selection ----------\n","            expandable_nodes = []\n","            for root in roots:\n","                node = root\n","                while node.is_expanded and not node.is_terminal() and node.children:\n","                    node = node.select_child(self.c_puct)\n","                if not node.is_terminal():\n","                    expandable_nodes.append(node)\n","            if not expandable_nodes:\n","                break\n","            # ---------- Batch Evaluation ----------\n","            batch_boards = torch.from_numpy(\n","                np.stack([board_to_tensor(node.game_state.board) for node in expandable_nodes])\n","            ).to(self.device)\n","            policy_logits_batch, value_batch = self.model(batch_boards)\n","            policy_logits_batch = policy_logits_batch.cpu()\n","            value_batch = value_batch.squeeze(1).cpu()\n","            # ---------- Expansion & Backup ----------\n","            for i, node in enumerate(expandable_nodes):\n","                node.expand(policy_logits_batch[i])\n","                node.backup(value_batch[i].item())\n","        # ---------- Extract final policy ----------\n","        policies = []\n","        for root in roots:\n","            policy = np.zeros(256)\n","            for move, child in root.children.items():\n","                (i, j), direction = move\n","                idx = i * 32 + j * 4 + direction\n","                policy[idx] = child.visit_count\n","            if policy.sum() > 0:\n","                policy /= policy.sum()\n","            policies.append(policy)\n","        return policies\n","\n","\n","def board_to_tensor(board):\n","    \"\"\"Convert board to tensor with 4 planes\"\"\"\n","    planes = np.zeros((4, 8, 8), dtype=np.float32)\n","    planes[0] = (board == 1).astype(np.float32)   # White pieces\n","    planes[1] = (board == 2).astype(np.float32)   # White kings\n","    planes[2] = (board == -1).astype(np.float32)  # Black pieces\n","    planes[3] = (board == -2).astype(np.float32)  # Black kings\n","    return planes\n","\n","class AlphaZeroTrainer:\n","    def __init__(self, model, lr=0.001):\n","        self.model = model\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.model.to(self.device)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n","\n","        # Replay buffer\n","        self.training_data = deque(maxlen=25000)\n","\n","    # -------- Self-play -------- #\n","    def self_play_game(self, num_simulations=150):\n","     game = FixedCheckers()\n","     mcts = MCTS(self.model, self.device)  # create once\n","\n","     states, mcts_policies, players = [], [], []\n","     count = 0\n","\n","     while count <= 100:\n","        count += 1\n","        # Temperature schedule: exploration for first 30 moves\n","        temperature = 1.0 if count <= 20   else 0.0\n","\n","        # Run MCTS (wrap game in list)\n","        policies = mcts.search([game], num_simulations=num_simulations)\n","        policy = policies[0]\n","\n","        # Get valid moves\n","        valid_moves_dict = game.get_valid_moves()\n","        valid_moves = game.get_moves(valid_moves_dict)\n","        if not valid_moves:\n","            break\n","\n","        # Extract policy probabilities for valid moves\n","        valid_policy = []\n","        for move in valid_moves:\n","            (i, j), direction = move\n","            idx = i * 32 + j * 4 + direction\n","            valid_policy.append(policy[idx])\n","        valid_policy = np.array(valid_policy)\n","        valid_policy = np.nan_to_num(valid_policy)  # replace any NaNs with 0\n","\n","        # Fallback if all probabilities are zero\n","        if valid_policy.sum() == 0:\n","            valid_policy = np.ones_like(valid_policy) / len(valid_policy)\n","\n","        # Choose move using temperature\n","        if temperature == 0:\n","            move_idx = np.argmax(valid_policy)\n","        else:\n","            valid_policy = valid_policy ** (1 / temperature)\n","            valid_policy /= valid_policy.sum()\n","            move_idx = np.random.choice(len(valid_policy), p=valid_policy)\n","\n","        move = valid_moves[move_idx]\n","\n","        # Record training data\n","        player_to_move = game.current_player\n","        board_tensor = board_to_tensor(game.board)\n","        states.append(board_tensor)\n","        mcts_policies.append(policy)\n","        players.append(player_to_move)\n","\n","        # Apply move\n","        game.apply_move(move)\n","        game.reverse()\n","        game.switch_player()\n","\n","    # Assign outcome\n","     result = game.EndGame()\n","     if result == 0:\n","         return []  # Empty list for draws\n","\n","     data = []\n","     for state, policy, player in zip(states, mcts_policies, players):\n","        value = result if player == 1 else -result\n","        data.append((state, policy, value))\n","     return data\n","\n","\n","\n","\n","\n","    def train_step(self, batch_size=2048):\n","     if len(self.training_data) < batch_size:\n","        return 0, 0\n","\n","     batch = random.sample(list(self.training_data), batch_size)\n","\n","    # Efficient stacking of NumPy arrays\n","     boards = torch.tensor(np.stack([x[0] for x in batch]), dtype=torch.float32).to(self.device)\n","     target_policies = torch.tensor(np.stack([x[1] for x in batch]), dtype=torch.float32).to(self.device)\n","     target_values = torch.tensor([[x[2]] for x in batch], dtype=torch.float32).to(self.device)\n","\n","    # Forward pass\n","     pred_logits, pred_values = self.model(boards)\n","\n","    # Policy loss (cross-entropy with MCTS distribution)\n","     pred_log_probs = F.log_softmax(pred_logits, dim=1)\n","     policy_loss = -(target_policies * pred_log_probs).sum(dim=1).mean()\n","\n","    # Value loss (MSE)\n","     value_loss = F.mse_loss(pred_values, target_values)\n","\n","     total_loss = policy_loss + value_loss\n","\n","    # Backward pass\n","     self.optimizer.zero_grad()\n","     total_loss.backward()\n","     torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","     self.optimizer.step()\n","\n","     return policy_loss.item(), value_loss.item()\n","\n","\n","    # -------- Main training loop -------- #\n","    def train(self, iterations=1000, games_per_iteration=10, train_steps_per_iteration=10):\n","        print(f\"Training on device: {self.device}\")\n","\n","        policy_losses = []\n","        value_losses = []\n","\n","        for iteration in tqdm(range(iterations)):\n","            # -------- Self-play --------\n","            for _ in range(games_per_iteration):\n","                # Anneal temperature: exploration in first 30 moves, greedy later\n","                examples = self.self_play_game(\n","                                         num_simulations=200,\n","                                          )\n","\n","                self.training_data.extend(examples)\n","\n","            # -------- Training --------\n","            total_policy_loss = 0\n","            total_value_loss = 0\n","\n","            for _ in range(train_steps_per_iteration):\n","                p_loss, v_loss = self.train_step()\n","                total_policy_loss += p_loss\n","                total_value_loss += v_loss\n","\n","            avg_policy_loss = total_policy_loss / max(train_steps_per_iteration, 1)\n","            avg_value_loss = total_value_loss / max(train_steps_per_iteration, 1)\n","\n","            policy_losses.append(avg_policy_loss)\n","            value_losses.append(avg_value_loss)\n","\n","            if iteration % 1 == 0:\n","                print(f\"Iteration {iteration}: Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}\")\n","                print(f\"Training data size: {len(self.training_data)}\")\n","\n","                # Save checkpoint\n","                torch.save(self.model.state_dict(), f'checkers_model_{iteration}.pth')\n","\n","        # -------- Plot training curves --------\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","        ax1.plot(policy_losses)\n","        ax1.set_title('Policy Loss')\n","        ax1.set_xlabel('Iteration')\n","        ax1.set_ylabel('Loss')\n","\n","        ax2.plot(value_losses)\n","        ax2.set_title('Value Loss')\n","        ax2.set_xlabel('Iteration')\n","        ax2.set_ylabel('Loss')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Save final model\n","        torch.save(self.model.state_dict(), 'checkers_final_model.pth')\n","\n","        return policy_losses, value_losses"],"metadata":{"id":"c1S8VQnSussE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D8spWlV8uspi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YSwqTLoCusm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PHePjNgwuskR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RHqRdwUNushN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tBhL_j9-usdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["self_play_game()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYz9sDabkVTC","executionInfo":{"status":"ok","timestamp":1759076953921,"user_tz":-330,"elapsed":78,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"3624e656-7b1a-4cb7-83a4-7e20eb2c1c2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(array([[ 0, -2,  0, -2,  0, -2,  0,  0],\n","         [ 0,  0,  0,  0, -1,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  1,  0],\n","         [ 0,  1,  0,  1,  0,  1,  0,  0],\n","         [-2,  0, -2,  0, -2,  0, -2,  0]]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0.]),\n","  -1)]"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["game.available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TiAa5Wu7KHID","executionInfo":{"status":"ok","timestamp":1759077021774,"user_tz":-330,"elapsed":59,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"3b299164-2e9a-478a-e7ca-05782141d0a7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(<function __main__.FixedCheckers.available.<locals>.<lambda>()>,\n","            {(5, 6): array([0., 1., 0., 0.])})"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["def self_play_game(   num_simulations=2, temperature=1.0):\n","        game = FixedCheckers()\n","        states, mcts_policies, players = [], [], []\n","\n","        while game.EndGame() == 0:\n","            # Run MCTS\n","            mcts = MCTS( model,  device)\n","            policy, _ = mcts.search(game, num_simulations=num_simulations)\n","\n","            # Get valid moves\n","            valid_moves_dict = game.get_valid_moves()\n","            valid_moves = game.get_moves(valid_moves_dict)\n","\n","\n","            valid_policy = []\n","            for move in valid_moves:\n","                (i, j), direction = move\n","                idx = i * 32 + j * 4 + direction\n","                valid_policy.append(policy[idx])\n","            valid_policy = np.array(valid_policy)\n","\n","            # Choose move\n","            if temperature == 0:\n","                move_idx = np.argmax(valid_policy)\n","            else:\n","                valid_policy = valid_policy ** (1 / temperature)\n","                valid_policy /= valid_policy.sum()\n","                move_idx = np.random.choice(len(valid_policy), p=valid_policy)\n","\n","            move = valid_moves[move_idx]\n","\n","            # Store training data\n","            player_to_move = game.current_player\n","            board_tensor =  game.board\n","            states.append(board_tensor)\n","            mcts_policies.append(policy)\n","            players.append(player_to_move)\n","\n","            # Apply move\n","            game.apply_move(move)\n","            game.reverse()\n","            game.switch_player()\n","\n","        # Assign outcome\n","        result = game.EndGame()  # +1 white win, -1 black win\n","        data = []\n","        for state, policy, player in zip(states, mcts_policies, players):\n","            value = result if player == 1 else -result\n","            data.append((state, policy, value))\n","\n","        return data"],"metadata":{"id":"aYX-nMw2kL-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","class FixedCheckers:\n","    \"\"\"Fixed version of your Checkers class\"\"\"\n","    def __init__(self):\n","        self.board = np.array([\n","            [ 0, -2,  0, -2,  0, -2,  0,  0],\n","         [ 0,  0,  0,  0, -1,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  1,  0,  0,  0,  1,  0],\n","         [ 0,  0,  0,  1,  0,  1,  0,  0],\n","         [-2,  0, -2,  0, -2,  0, -2,  0]\n","        ], dtype=int)\n","        self.current_player = 1\n","    def switch_player(self):\n","        \"\"\"Explicitly switch the current player\"\"\"\n","        self.current_player *= -1\n","    def reverse(self):\n","\n","      flipped = [row[::-1] for row in self.board[::-1]]\n","\n","      mapping = {1: -1, 0: 0, 2: -2, -1: 1,  }\n","      self.board= np.array([[mapping.get(cell, cell) for cell in row] for row in flipped])\n","\n","      return self.board\n","\n","    def board_to_tensor(self):\n","\n","            return board_to_tensor(self.board )\n","\n","\n","    def copy(self):\n","        \"\"\"Create a copy of the game state\"\"\"\n","        new_game = FixedCheckers()\n","        new_game.board = self.board.copy()\n","        new_game.current_player = self.current_player\n","        return new_game\n","\n","    def get_valid_moves(self, player=1):\n","        \"\"\"Get valid moves for current player\"\"\"\n","        if player is None:\n","            player = self.current_player\n","        return self.available(player)\n","\n","    def available(self, player=1):\n","        d = defaultdict(lambda: np.zeros(4))\n","\n","\n","\n","        if player == 1:  # white pieces\n","            for i in range(len(self.board)):\n","                for j in range(len(self.board)):\n","                    if self.board[i][j] != 0 and self.board[i][j] >= player:\n","                        # Forward left\n","                        if i+1 < 8 and j-1 >= 0:\n","                            if self.board[i+1][j-1] == 0:\n","                                d[i, j][0] = 1\n","                            elif self.board[i+1][j-1] <  0:\n","                                if i+2 < 8 and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                                    d[i, j][0] = 1\n","\n","                        if i+1 < 8 and j+1 < 8:\n","                            if self.board[i+1][j+1] == 0:\n","                                d[i, j][1] = 1\n","                            elif self.board[i+1][j+1] <  0:\n","                                if i+2 < 8 and j+2 <= 7 and self.board[i+2][j+2] == 0:\n","                                    d[i,j][1] = 1\n","\n","                    if self.board[i][j] != 0 and self.board[i][j] > player:\n","                        # Backward left\n","                        if i-1 >= 0 and j-1 >= 0:\n","                            if self.board[i-1][j-1] == 0:\n","                                d[i, j][2] = 1\n","                            elif self.board[i-1][j-1]  < 0:\n","                                if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                                    d[i, j][2] = 1\n","                        # Backward right\n","                        if i-1 >= 0 and j+1 < 8:\n","                            if self.board[i-1][j+1] == 0:\n","                                d[i, j][3] = 1\n","                            elif self.board[i-1][j+1]  < 0:\n","                                if i-2 >= 0 and j+2 < 8 and self.board[i-2][j+2] == 0:\n","                                    d[i, j][3] = 1\n","        return d\n","\n","\n","    def apply_move(self, action, player=1 ):\n","\n","     i, j = action[0]\n","     move = action[1]\n","\n","     if player == 1:  # White player\n","        if move == 0:  # forward-left\n","            if self.board[i+1][j-1] == 0:\n","                self.board[i+1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j-1] == 1:\n","                    self.board[i+1][j-1] += 1\n","\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j-1]\n","\n","                self.board[i+1][j-1] = 0\n","                self.board[i+2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j-2] == 1:\n","                    self.board[i+2][j-2] += 1\n","\n","                act = self.Possible_cap(i+2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 1:  # forward-right\n","            if self.board[i+1][j+1] == 0:\n","                self.board[i+1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j+1] == 1:\n","                    self.board[i+1][j+1] += 1\n","                    # Extra point for king promotion\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j+1]\n","                   # Remove from black's score\n","\n","                self.board[i+1][j+1] = 0\n","                self.board[i+2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j+2] == 1:\n","                    self.board[i+2][j+2] += 1\n","\n","                act = self.Possible_cap(i+2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 2:  # backward-left\n","            if self.board[i-1][j-1] == 0:\n","                self.board[i-1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j-1]\n","\n","\n","                self.board[i-1][j-1] = 0\n","                self.board[i-2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 3:  # backward-right\n","            if self.board[i-1][j+1] == 0:\n","                self.board[i-1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j+1]\n","\n","\n","                self.board[i-1][j+1] = 0\n","                self.board[i-2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","     return self.board\n","\n","    def Possible_cap(self, i, j, player=1):\n","     d   = defaultdict(lambda: np.zeros(4))\n","     n = 8\n","\n","     if self.board[i][j] >= 1:\n","            # Forward left\n","            if i+2 < n and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                if self.board[i+1][j-1] != 0 and self.board[i+1][j-1] <= 0:\n","                    d[i, j][0] = 1\n","            # Forward right\n","            if i+2 < n and j+2 < n and self.board[i+2][j+2] == 0:\n","                if self.board[i+1][j+1] != 0 and self.board[i+1][j+1] <= 0:\n","                    d[i, j][1] = 1\n","     if self.board[i][j] > 1:\n","\n","            if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                if self.board[i-1][j-1] != 0 and self.board[i-1][j-1] <= 0:\n","                    d[i, j][2] = 1\n","            # Backward right\n","            if i-2 >= 0 and j+2 < n and self.board[i-2][j+2] == 0:\n","                if self.board[i-1][j+1] != 0 and self.board[i-1][j+1] <= 0:\n","                    d[i, j][3] = 1\n","\n","     return self.get_moves(d)\n","\n","\n","\n","    def get_moves(self, d):\n","        \"\"\"Convert move dict to list\"\"\"\n","        move_list = []\n","        for pos, arr in d.items():\n","            for i, val in enumerate(arr):\n","                if val == 1:\n","                    move_list.append((pos, i))\n","        return move_list\n","\n","\n","    def EndGame(self):\n","        \"\"\"Check if game has ended\"\"\"\n","        white_pieces = np.sum(self.board > 0)\n","        black_pieces = np.sum(self.board < 0)\n","\n","        if white_pieces == 0:\n","            return -1 # Black wins\n","        elif black_pieces == 0:\n","            return 1   # White wins\n","\n","        # Check if current player has moves\n","        moves = self.get_moves(self.get_valid_moves( ))\n","        if not moves:\n","            return - 1  # Current player loses\n","\n","        return 0  # Game continues\n"],"metadata":{"id":"CI3O1WzVvhI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out = F.relu(out + residual)  # Skip connection + activation\n","        return out\n","class  CheckersCNN(nn.Module):\n","    def __init__(self, num_res_blocks=8, board_channels=4, torso_channels=512):\n","        super().__init__()\n","\n","        self.conv_in = nn.Conv2d(board_channels, torso_channels, kernel_size=3, padding=1)\n","        self.bn_in = nn.BatchNorm2d(torso_channels)\n","\n","        self.res_blocks = nn.ModuleList([\n","            ResidualBlock(torso_channels) for _ in range(num_res_blocks)\n","        ])\n","\n","        # OPTIMIZED: Policy head (reduced from 1024 to 512 actions)\n","        self.policy_conv = nn.Conv2d(torso_channels, 12, kernel_size=1)  # 16→12 channels\n","        self.policy_bn = nn.BatchNorm2d(12)\n","        self.policy_fc = nn.Linear(12 * 8 * 8, 256)  # 1024→512 actions (still plenty)\n","\n","        # OPTIMIZED: Slightly smaller value head (still deep)\n","        self.value_conv = nn.Conv2d(torso_channels, 6, kernel_size=1)  # 8→6 channels\n","        self.value_bn = nn.BatchNorm2d(6)\n","        self.value_fc1 = nn.Linear(6 * 8 * 8, 256)\n","        self.value_fc2 = nn.Linear(256, 64)  # 128→64 (smaller bottleneck)\n","        self.value_fc3 = nn.Linear(64, 16)   # 32→16 (smaller final layer)\n","        self.value_fc4 = nn.Linear(16, 1)\n","\n","        # ADD: Dropout for regularization (optional)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        # Feature extraction\n","        x = F.relu(self.bn_in(self.conv_in(x)))\n","\n","        for res_block in self.res_blocks:\n","            x = res_block(x)\n","\n","        # Policy head\n","        policy = F.relu(self.policy_bn(self.policy_conv(x)))\n","        policy = policy.view(policy.size(0), -1)\n","        policy = self.dropout(policy)  # Optional: add dropout\n","        policy = self.policy_fc(policy)\n","        policy = F.log_softmax(policy, dim=1)\n","\n","        # Value head\n","        value = F.relu(self.value_bn(self.value_conv(x)))\n","        value = value.view(value.size(0), -1)\n","        value = F.relu(self.value_fc1(value))\n","        value = self.dropout(value)  # Optional: add dropout\n","        value = F.relu(self.value_fc2(value))\n","        value = F.relu(self.value_fc3(value))\n","        value = torch.tanh(self.value_fc4(value))\n","\n","        return policy, value\n"],"metadata":{"id":"MjmQcNsKBQ5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","\n","\n","# Example usage:\n","game = FixedCheckers()\n","board_tensor = board_to_tensor(game.board, game.current_player)\n","\n","# Add batch dimension for PyTorch\n","board_tensor = torch.tensor(board_tensor).unsqueeze(0)  # shape: (1, 4, 8, 8)\n","\n","# Initialize the model\n","model = CheckersCNN()\n","policy, value = model(board_tensor)\n","\n","print(\"Policy shape:\", policy.shape)  # (1, 512)\n","print(\"Value:\", value.item())          # scalar between -1 and 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"ut1NjyU_BQtj","executionInfo":{"status":"error","timestamp":1759076942293,"user_tz":-330,"elapsed":208,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"bf6cc274-0b9e-40b8-e26f-d9dfbf4b8dd8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"board_to_tensor() takes 1 positional argument but 2 were given","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-139547385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFixedCheckers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mboard_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboard_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Add batch dimension for PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: board_to_tensor() takes 1 positional argument but 2 were given"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","def select_best_move(policy_logits, valid_moves):\n","\n","\n","    # Create mask\n","    mask = torch.zeros_like(policy_logits)\n","    move_map = {}  # index -> (pos, direction)\n","\n","    for (i, j), moves in valid_moves.items():\n","        for d, is_valid in enumerate(moves):\n","            if is_valid == 1:\n","                idx = (i * 8 + j) * 4 + d\n","                mask[idx] = 1\n","                move_map[idx] = ((i, j), d)  # FIX: no .item()\n","\n","    # Apply mask\n","    masked_logits = torch.where(mask == 1, policy_logits, torch.tensor(-1e9, device=policy_logits.device))\n","\n","    # Softmax to get probabilities\n","    probs = F.softmax(masked_logits, dim=-1)\n","\n","    # Best index\n","    best_idx = torch.argmax(probs).item()\n","    best_move = move_map[best_idx]\n","    best_prob = probs[best_idx].item()\n","\n","    return best_move, best_prob\n","\n"],"metadata":{"id":"BgqtWQ7o70uq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["policy_logits = torch.randn(256)\n","valid_moves = game.available(player=game.current_player)\n","\n","best_move, prob = select_best_move(policy_logits, valid_moves)\n","\n","print(\"Best move:\", best_move, \"with probability:\", prob)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_4pLz9n6_j7","executionInfo":{"status":"ok","timestamp":1759076942960,"user_tz":-330,"elapsed":164,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"9425e59d-ed0f-4b5a-e910-82bbcb615f3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best move: ((5, 2), 0) with probability: 0.667500376701355\n"]}]},{"cell_type":"code","source":["class Node:\n","    def __init__(self, game_state, parent=None, action_taken=None, prior=0):\n","        self.game_state = game_state.copy()\n","        self.parent = parent\n","        self.action_taken = action_taken\n","        self.prior = prior\n","\n","        self.children = {}\n","        self.visit_count = 0\n","        self.value_sum = 0\n","        self.is_expanded = False\n","\n","    def is_terminal(self):\n","        return self.game_state.EndGame() != 0\n","\n","    def expand(self, policy_logits):\n","\n","     if self.is_expanded:\n","        return\n","\n","    # 1. Get valid moves for current game state\n","     valid_moves_dict = self.game_state.get_valid_moves()\n","     valid_moves = self.game_state.get_moves(valid_moves_dict)\n","     if not valid_moves:\n","        self.is_expanded = True\n","        return\n","\n","    # 2. Create mask for valid moves\n","     mask = torch.zeros_like(policy_logits)\n","     move_map = {}  # Maps index in policy to actual move\n","     for move in valid_moves:\n","        (i, j), direction = move\n","        idx = i * 32 + j * 4 + direction\n","        mask[idx] = 1\n","        move_map[idx ] = move  # store mapping for later\n","\n","    # 3. Apply softmax on logits\n","     if policy_logits.dim() == 1:  # single board\n","            policy_probs = F.softmax(policy_logits, dim=0)\n","     else:  # batch (should not happen in MCTS, but safe)\n","            policy_probs = F.softmax(policy_logits, dim=1)[0]\n","\n","    # 4. Mask invalid moves and renormalize\n","     masked_probs = policy_probs * mask\n","     if masked_probs.sum() > 0:\n","        masked_probs /= masked_probs.sum()\n","     else:\n","        # Fallback: all moves are invalid (should not happen)\n","        masked_probs = mask / mask.sum()\n","\n","    # 5. Create child nodes for all valid moves\n","     for idx, prob in enumerate(masked_probs):\n","        if prob > 0:\n","            move = move_map[idx]\n","            new_game = self.game_state.copy()\n","            new_game.apply_move(move)\n","            new_game.reverse()  # Switch player perspective\n","            child = Node(new_game, parent=self, action_taken=move, prior=prob.item())\n","            self.children[move] = child\n","\n","     self.is_expanded = True\n","\n","    def select_child(self, c_puct=1.0):\n","        \"\"\"Select child with highest UCB score\"\"\"\n","        best_score = -float('inf')\n","        best_child = None\n","\n","        for child in self.children.values():\n","            q_value = 0 if child.visit_count == 0 else child.value_sum / child.visit_count\n","            u_value = c_puct * child.prior * math.sqrt(self.visit_count) / (1 + child.visit_count)\n","            score = q_value + u_value\n","\n","            if score > best_score:\n","                best_score = score\n","                best_child = child\n","\n","        return best_child\n","\n","    def backup(self, value):\n","        \"\"\"Backup value through the tree\"\"\"\n","        self.visit_count += 1\n","        self.value_sum += value\n","\n","        if self.parent:\n","            self.parent.backup(-value)\n"],"metadata":{"id":"0rbIFgvZLzwH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Assuming FixedCheckers is already defined\n","game = FixedCheckers()\n","\n","# Create random policy logits (size 256)\n","policy_logits = torch.randn(256)\n","\n","# Create root node\n","root = Node(game_state=game)\n","\n","# Expand root node with policy logits\n","root.expand(policy_logits)\n","\n","# Print some info\n","print(\"Is root expanded?\", root.is_expanded)\n","print(\"Number of children created:\", len(root.children))\n","\n","# List child moves and their prior probabilities\n","for move, child in root.children.items():\n","    print(\"Move:\", move, \"Prior probability:\", child.prior)\n","\n","# Optional: select best child using UCB\n","best_child = root.select_child(c_puct=1.0)\n","print(\"Best child selected:\", best_child.action_taken, \"with prior:\", best_child.prior)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBA3v4xKXWxd","executionInfo":{"status":"ok","timestamp":1759076942979,"user_tz":-330,"elapsed":10,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"676a740a-fc4d-44fc-e08f-1bb111625436"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Is root expanded? True\n","Number of children created: 2\n","Move: ((5, 2), 0) Prior probability: 0.6989935040473938\n","Move: ((5, 6), 1) Prior probability: 0.3010064959526062\n","Best child selected: ((5, 2), 0) with prior: 0.6989935040473938\n"]}]},{"cell_type":"code","source":["def print_tree(node, depth=0):\n","    indent = \"  \" * depth\n","    move = node.action_taken if node.action_taken else \"Root\"\n","    print(f\"{indent}- Move: {move}, Visits: {node.visit_count}, Value Sum: {node.value_sum:.2f}, Prior: {node.prior:.4f}\")\n","    for child in node.children.values():\n","        print_tree(child, depth + 1)\n","\n","# Example usage:\n","print_tree(root)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEs1TPnrX2gs","executionInfo":{"status":"ok","timestamp":1759076943180,"user_tz":-330,"elapsed":10,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"03e82f11-4265-4454-d465-259de2d5421d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["- Move: Root, Visits: 0, Value Sum: 0.00, Prior: 0.0000\n","  - Move: ((5, 2), 0), Visits: 0, Value Sum: 0.00, Prior: 0.6990\n","  - Move: ((5, 6), 1), Visits: 0, Value Sum: 0.00, Prior: 0.3010\n"]}]},{"cell_type":"code","source":["\n","\n","class MCTS:\n","    def __init__(self, model, device, c_puct=1.0):\n","        self.model = model\n","        self.device = device\n","        self.c_puct = c_puct\n","\n","    def search(self, game_state, num_simulations=100):\n","        root = Node(game_state)\n","\n","        for _ in range(num_simulations):\n","            node = root\n","\n","            # -------- Selection --------\n","            while node.is_expanded and not node.is_terminal():\n","                node = node.select_child(self.c_puct)\n","\n","            # -------- Evaluation --------\n","            if node.is_terminal():\n","                value = node.game_state.EndGame()\n","            else:\n","                # Neural network prediction\n","                board_tensor = torch.tensor(\n","                    node.game_state.board_to_tensor(), dtype=torch.float32\n","                ).unsqueeze(0).to(self.device)  # shape: (1, C, 8, 8)\n","\n","                with torch.no_grad():\n","                    policy_logits, value_tensor = self.model(board_tensor)\n","                    policy_logits = policy_logits.squeeze(0)  # (256,)\n","                    value = value_tensor.item()\n","\n","                # -------- Expansion --------\n","                node.expand(policy_logits)\n","\n","            # -------- Backup --------\n","            node.backup(value)\n","\n","        # -------- Build policy from visit counts --------\n","        valid_moves_dict = game_state.get_valid_moves()\n","        valid_moves = game_state.get_moves(valid_moves_dict)\n","        policy = np.zeros(256)\n","\n","        for move in valid_moves:\n","            if move in root.children:\n","                (i, j), direction = move\n","                idx = i * 32 + j * 4 + direction\n","                policy[idx] = root.children[move].visit_count\n","\n","        if policy.sum() > 0:\n","            policy /= policy.sum()\n","\n","        return policy, root.value_sum / max(root.visit_count, 1)\n"],"metadata":{"id":"ulHTPf0fHX_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# ------------------------------\n","# 1. Initialize CNN and device\n","# ------------------------------\n","device = \"cpu\"\n","model = CheckersCNN(num_res_blocks=4)  # smaller for testing\n","model.to(device)\n","model.eval()  # inference mode\n","\n","# ------------------------------\n","# 2. Initialize checkers board\n","# ------------------------------\n","game = FixedCheckers()\n","\n","# ------------------------------\n","# 3. Initialize MCTS\n","# ------------------------------\n","mcts = MCTS(model=model, device=device, c_puct=1.0)\n","\n","# ------------------------------\n","# 4. Run MCTS search\n","# ------------------------------\n","num_simulations = 10  # small number to test\n","policy, root_value = mcts.search(game, num_simulations=num_simulations)\n","\n","# ------------------------------\n","# 5. Print results\n","# ------------------------------\n","print(\"Root value estimate:\", root_value)\n","print(\"Policy visit counts (non-zero moves):\")\n","for idx, prob in enumerate(policy):\n","    if prob > 0:\n","        i = idx // 32\n","        j = (idx % 32) // 4\n","        direction = idx % 4\n","        print(f\"Move: (i={i}, j={j}, dir={direction}) -> probability {prob:.3f}\")\n","\n","# ------------------------------\n","# 6. Best move according to MCTS\n","# ------------------------------\n","best_idx = np.argmax(policy)\n","best_i = best_idx // 32\n","best_j = (best_idx % 32) // 4\n","best_direction = best_idx % 4\n","print(\"Best move:\", (best_i, best_j, best_direction))\n","\n","# ------------------------------\n","# 7. Apply the best move and show new board\n","# ------------------------------\n","best_move = ((best_i, best_j), best_direction)\n","game.apply_move(best_move)\n","print(\"Board after best move:\")\n","print(game.board)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-TgaOteGLYuY","executionInfo":{"status":"ok","timestamp":1759076943273,"user_tz":-330,"elapsed":61,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"d8b34376-b8ad-4e10-b71b-4b4edf0f4761"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Root value estimate: 0.8911326974630356\n","Policy visit counts (non-zero moves):\n","Move: (i=5, j=2, dir=0) -> probability 0.556\n","Move: (i=5, j=6, dir=1) -> probability 0.444\n","Best move: (np.int64(5), np.int64(2), np.int64(0))\n","Board after best move:\n","[[ 0 -2  0 -2  0 -2  0  0]\n"," [ 0  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0  1  0  1  0  1  0  0]\n"," [-2  0 -2  0 -2  0 -2  0]]\n"]}]},{"cell_type":"code","source":["policy"],"metadata":{"id":"l_9UuGM40MJq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759076943292,"user_tz":-330,"elapsed":15,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"037a73d8-a263-41a0-90fe-b22cbc2d711a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.55555556, 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.44444444, 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        ])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["def self_play_game(   num_simulations=2, temperature=1.0):\n","        game = FixedCheckers()\n","        states, mcts_policies, players = [], [], []\n","\n","        while game.EndGame() == 0:\n","            # Run MCTS\n","            mcts = MCTS( model,  device)\n","            policy, _ = mcts.search(game, num_simulations=num_simulations)\n","\n","            # Get valid moves\n","            valid_moves_dict = game.get_valid_moves()\n","            valid_moves = game.get_moves(valid_moves_dict)\n","\n","\n","            valid_policy = []\n","            for move in valid_moves:\n","                (i, j), direction = move\n","                idx = i * 32 + j * 4 + direction\n","                valid_policy.append(policy[idx])\n","            valid_policy = np.array(valid_policy)\n","\n","            # Choose move\n","            if temperature == 0:\n","                move_idx = np.argmax(valid_policy)\n","            else:\n","                valid_policy = valid_policy ** (1 / temperature)\n","                valid_policy /= valid_policy.sum()\n","                move_idx = np.random.choice(len(valid_policy), p=valid_policy)\n","\n","            move = valid_moves[move_idx]\n","\n","            # Store training data\n","            player_to_move = game.current_player\n","            board_tensor =  game.board\n","            states.append(board_tensor)\n","            mcts_policies.append(policy)\n","            players.append(player_to_move)\n","\n","            # Apply move\n","            game.apply_move(move)\n","            game.reverse()\n","            game.switch_player()\n","\n","        # Assign outcome\n","        result = game.EndGame()  # +1 white win, -1 black win\n","        data = []\n","        for state, policy, player in zip(states, mcts_policies, players):\n","            value = result if player == 1 else -result\n","            data.append((state, policy, value))\n","\n","        return data"],"metadata":{"id":"DzlbF9k1LTLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["self_play_game()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcykQcRlLlU-","executionInfo":{"status":"ok","timestamp":1759076943349,"user_tz":-330,"elapsed":13,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"8e21e5db-5bc7-4af4-808a-f1126035fce8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(array([[ 0, -2,  0, -2,  0, -2,  0,  0],\n","         [ 0,  0,  0,  0, -1,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  0,  0],\n","         [ 0,  0,  0,  0,  0,  0,  1,  0],\n","         [ 0,  1,  0,  1,  0,  1,  0,  0],\n","         [-2,  0, -2,  0, -2,  0, -2,  0]]),\n","  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0.]),\n","  -1)]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":[],"metadata":{"id":"CENEuiv1j-nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7XzlWr6PPHuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["game.available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"ImxZ7msGOcGp","executionInfo":{"status":"error","timestamp":1759064869050,"user_tz":-330,"elapsed":404,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"71765ee8-82ec-4402-8acb-ab78315b51e0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'game' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2248011109.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'game' is not defined"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import random\n","import numpy as np\n","from collections import deque\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","# ---------------- Helper ---------------- #\n","def board_to_tensor(board):\n","    \"\"\"Convert board to tensor with 4 planes\"\"\"\n","    planes = np.zeros((4, 8, 8), dtype=np.float32)\n","    planes[0] = (board == 1).astype(np.float32)   # White pieces\n","    planes[1] = (board == 2).astype(np.float32)   # White kings\n","    planes[2] = (board == -1).astype(np.float32)  # Black pieces\n","    planes[3] = (board == -2).astype(np.float32)  # Black kings\n","    return planes\n","\n","# ---------------- AlphaZero Trainer ---------------- #\n","class AlphaZeroTrainer:\n","    def __init__(self, model, lr=0.001):\n","        self.model = model\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.model.to(self.device)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n","\n","        # Replay buffer\n","        self.training_data = deque(maxlen=10000)\n","\n","    # -------- Self-play -------- #\n","    def self_play_game(self, num_simulations=100, temperature=1.0):\n","        game = FixedCheckers()\n","        states, mcts_policies, players = [], [], []\n","\n","        while game.EndGame() == 0:\n","            # Run MCTS\n","            mcts = MCTS(self.model, self.device)\n","            policy, _ = mcts.search(game, num_simulations=num_simulations)\n","\n","            # Get valid moves\n","            valid_moves_dict = game.get_valid_moves()\n","            valid_moves = game.get_moves(valid_moves_dict)\n","\n","            if not valid_moves:\n","              break\n","            valid_policy = []\n","            for move in valid_moves:\n","                (i, j), direction = move\n","                idx = i * 32 + j * 4 + direction\n","                valid_policy.append(policy[idx])\n","            valid_policy = np.array(valid_policy)\n","\n","            # Choose move\n","            if temperature == 0:\n","                move_idx = np.argmax(valid_policy)\n","            else:\n","                valid_policy = valid_policy ** (1 / temperature)\n","                valid_policy /= valid_policy.sum()\n","                move_idx = np.random.choice(len(valid_policy), p=valid_policy)\n","\n","            move = valid_moves[move_idx]\n","\n","            # Store training data\n","            player_to_move = game.current_player\n","            board_tensor = board_to_tensor(game.board)\n","            states.append(board_tensor)\n","            mcts_policies.append(policy)\n","            players.append(player_to_move)\n","\n","            # Apply move\n","            game.apply_move(move)\n","            game.reverse()\n","            game.switch_player()\n","\n","        # Assign outcome\n","        result = game.EndGame()  # +1 white win, -1 black win\n","        data = []\n","        for state, policy, player in zip(states, mcts_policies, players):\n","            value = result if player == 1 else -result\n","            data.append((state, policy, value))\n","\n","        return data\n","\n","\n","    def train_step(self, batch_size=32):\n","     if len(self.training_data) < batch_size:\n","        return 0, 0\n","\n","     batch = random.sample(list(self.training_data), batch_size)\n","\n","     boards = torch.tensor([x[0] for x in batch], dtype=torch.float32).to(self.device)\n","     target_policies = torch.tensor([x[1] for x in batch], dtype=torch.float32).to(self.device)\n","     target_values = torch.tensor([[x[2]] for x in batch], dtype=torch.float32).to(self.device)\n","\n","    # Forward pass\n","     pred_logits, pred_values = self.model(boards)  # pred_logits: raw logits\n","\n","    # Policy loss (KL divergence)\n","     pred_log_probs = F.log_softmax(pred_logits, dim=1)  # log probabilities\n","     policy_loss = F.kl_div(pred_log_probs, target_policies, reduction='batchmean')\n","\n","    # Value loss (MSE)\n","     value_loss = F.mse_loss(pred_values, target_values)\n","\n","     # Total loss\n","     total_loss = policy_loss + value_loss\n","\n","    # Backward pass\n","     self.optimizer.zero_grad()\n","     total_loss.backward()\n","     torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","     self.optimizer.step()\n","\n","     return policy_loss.item(), value_loss.item()\n","\n","\n","    # -------- Main training loop -------- #\n","    def train(self, iterations=1000, games_per_iteration=10, train_steps_per_iteration=10):\n","        print(f\"Training on device: {self.device}\")\n","\n","        policy_losses = []\n","        value_losses = []\n","\n","        for iteration in tqdm(range(iterations)):\n","            # -------- Self-play --------\n","            for _ in range(games_per_iteration):\n","                # Anneal temperature: exploration in first 30 moves, greedy later\n","                examples = self.self_play_game(num_simulations=50, temperature=1.0)\n","                self.training_data.extend(examples)\n","\n","            # -------- Training --------\n","            total_policy_loss = 0\n","            total_value_loss = 0\n","\n","            for _ in range(train_steps_per_iteration):\n","                p_loss, v_loss = self.train_step()\n","                total_policy_loss += p_loss\n","                total_value_loss += v_loss\n","\n","            avg_policy_loss = total_policy_loss / max(train_steps_per_iteration, 1)\n","            avg_value_loss = total_value_loss / max(train_steps_per_iteration, 1)\n","\n","            policy_losses.append(avg_policy_loss)\n","            value_losses.append(avg_value_loss)\n","\n","            if iteration % 1 == 0:\n","                print(f\"Iteration {iteration}: Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}\")\n","                print(f\"Training data size: {len(self.training_data)}\")\n","\n","                # Save checkpoint\n","                torch.save(self.model.state_dict(), f'checkers_model_{iteration}.pth')\n","\n","        # -------- Plot training curves --------\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","        ax1.plot(policy_losses)\n","        ax1.set_title('Policy Loss')\n","        ax1.set_xlabel('Iteration')\n","        ax1.set_ylabel('Loss')\n","\n","        ax2.plot(value_losses)\n","        ax2.set_title('Value Loss')\n","        ax2.set_xlabel('Iteration')\n","        ax2.set_ylabel('Loss')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Save final model\n","        torch.save(self.model.state_dict(), 'checkers_final_model.pth')\n","\n","        return policy_losses, value_losses\n"],"metadata":{"id":"DLGzs7T_Ffwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Usage\n","if __name__ == \"__main__\":\n","    # Create model\n","    model = CheckersCNN()\n","\n","    # Create trainer\n","    trainer = AlphaZeroTrainer(model, lr=0.001)\n","\n","    # Train\n","    policy_losses, value_losses = trainer.train(\n","        iterations=5,\n","        games_per_iteration=2,\n","        train_steps_per_iteration=10\n","    )\n","\n","    print(\"Training completed!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":739},"id":"CRf4O2NyHXxC","executionInfo":{"status":"ok","timestamp":1759046841648,"user_tz":-330,"elapsed":416628,"user":{"displayName":"soul less","userId":"12195366273789735218"}},"outputId":"0597cc4c-20ef-4176-adc9-f42630ade93a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipython-input-4146507276.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  boards = torch.tensor([x[0] for x in batch], dtype=torch.float32).to(self.device)\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 0: Policy Loss: 2.2079, Value Loss: 0.8339\n","Training data size: 142\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 2/5 [02:35<03:49, 76.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 1: Policy Loss: 1.2776, Value Loss: 0.5213\n","Training data size: 261\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 3/5 [04:01<02:41, 80.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 2: Policy Loss: 1.1084, Value Loss: 0.5073\n","Training data size: 410\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 4/5 [05:27<01:22, 82.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 3: Policy Loss: 0.9957, Value Loss: 0.3874\n","Training data size: 558\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [06:50<00:00, 82.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 4: Policy Loss: 0.8573, Value Loss: 0.6274\n","Training data size: 706\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x500 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training completed!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"sDLUmSdAV4z0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nqKNNrbiV4wv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8dDEdYTJV4gW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mAnnj4M7V4Qu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"594nsJuIV3sg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    model = CheckersCNN()\n","    # Provide path to your saved checkpoint here\n","    checkpoint_path = \"checkers_model_5.pth\"  # <-- change this to your file\n","    trainer = AlphaZeroTrainer(model, lr=0.001, checkpoint=checkpoint_path)\n","\n","    # Continue training\n","    policy_losses, value_losses = trainer.train(\n","        iterations=5,\n","        games_per_iteration=10,\n","        train_steps_per_iteration=10\n","    )"],"metadata":{"id":"545q20wPHXuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OvSs3TrgBQQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"elapsed":52,"status":"error","timestamp":1758952512624,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"2-DtwVlIB1D_","outputId":"8616a937-38b9-4674-ba3c-2c30a445b98f"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Checkers' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1019766871.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Run the test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtest_node_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1019766871.py\u001b[0m in \u001b[0;36mtest_node_class\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# --- Initialize game ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Checkers' is not defined"]}],"source":["def test_node_class():\n","    import numpy as np\n","\n","    # --- Initialize game ---\n","    game = Checkers()\n","    game.set_initial_state()\n","\n","    # --- Node arguments ---\n","    args = {\"C\": 1.4, \"num_searches\": 0}  # num_searches not used here\n","\n","    # --- Create root node ---\n","    root_state = game.board.copy()\n","    root_node = Node(game, args, root_state)\n","\n","    print(\"Root expandable moves:\", root_node.expandable_moves)\n","\n","    # --- Expand first 3 moves ---\n","    for _ in range(min(3, len(root_node.expandable_moves))):\n","        child = root_node.expand()\n","        print(\"Expanded action:\", child.action_taken)\n","        print(\"Child board state:\\n\", child.state)\n","\n","        # --- Simulate from child ---\n","        value = child.simulate()\n","        print(\"Simulation value from child:\", value)\n","\n","        # --- Backpropagate value ---\n","        child.backpropagate(value)\n","        print(\"After backpropagate: root visit count =\", root_node.visit_count)\n","        print(\"Root value sum =\", root_node.value_sum)\n","        print(\"-----\")\n","\n","    print(\"Final root node children count:\", len(root_node.children))\n","    for c in root_node.children:\n","        print(\"Child action:\", c.action_taken, \"Visit count:\", c.visit_count, \"Value sum:\", c.value_sum)\n","\n","# Run the test\n","test_node_class()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1758741804240,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"Km0CdxrhKi2u","outputId":"275fc729-816f-4c1c-ae79-481dc997e9c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action probabilities from MCTS search:\n","Move ((2, 7), 0): 0.150\n","Move ((2, 5), 1): 0.150\n","Move ((2, 5), 0): 0.150\n","Move ((2, 3), 1): 0.150\n","Move ((2, 3), 0): 0.150\n","Move ((2, 1), 1): 0.150\n","Move ((2, 1), 0): 0.100\n"]}],"source":["import numpy as np\n","from collections import defaultdict\n","import math\n","\n","def test_mcts_with_checkers():\n","    # --- Create game instance ---\n","    game = Checkers()\n","    game.set_initial_state()\n","\n","    # --- MCTS arguments ---\n","    args = {\n","        \"num_searches\": 20,  # number of simulations\n","        \"C\": 1.4             # exploration constant\n","    }\n","\n","    # --- Initialize MCTS ---\n","    mcts = MCTS(game, args)\n","\n","    # --- Run search ---\n","    state = game.board.copy()\n","    action_probs = mcts.search(state)\n","\n","    # --- Print results ---\n","    print(\"Action probabilities from MCTS search:\")\n","    for action, prob in action_probs.items():\n","        print(f\"Move {action}: {prob:.3f}\")\n","\n","# Run the test\n","test_mcts_with_checkers()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dJq72nEAmwC"},"outputs":[],"source":["class Node:\n","    def __init__(self, game, args, state, parent=None, action_taken=None):\n","        self.game = game\n","        self.args = args\n","        self.state = state\n","        self.parent = parent\n","        self.action_taken = action_taken\n","\n","        self.children = []\n","        self.expandable_moves =list(self.game.get_moves(self.game.get_valid_moves(self.state)))\n","\n","        self.visit_count = 0\n","        self.value_sum = 0\n","\n","    def is_fully_expanded(self):\n","        return len(self.expandable_moves) == 0 and len(self.children) > 0\n","\n","    def select(self):\n","        best_child = None\n","        best_ucb = -np.inf\n","\n","        for child in self.children:\n","            ucb = self.get_ucb(child)\n","            if ucb > best_ucb:\n","                best_child = child\n","                best_ucb = ucb\n","\n","        return best_child\n","\n","    def get_ucb(self, child):\n","        q_value = child.value_sum / (child.visit_count + 1e-6)  # avoid div by zero\n","        return q_value + self.args[\"C\"] * math.sqrt(\n","            math.log(self.visit_count + 1) / (child.visit_count + 1e-6)\n","        )\n","\n","\n","\n","    def expand(self):\n","        if not self.expandable_moves:\n","            return self\n","\n","        action = self.expandable_moves.pop()\n","        child_state = self.state.copy()\n","        self.game.board = child_state.copy()  # sync\n","        child_state = self.game.apply_move(action)\n","\n","        child_state = self.game.get_opponent()  # switch perspective\n","        child = Node(self.game, self.args, child_state, self, action)\n","        self.children.append(child)\n","        return child\n","\n","    def simulate(self):\n","    # Check terminal state\n","     if self.game.EndGame() != 0:\n","         return self.game.EndGame()\n","\n","     rollout_player = 1\n","     max_steps = 100  # prevent infinite loops\n","     for _ in range(max_steps):\n","        moves = self.game.get_moves(self.game.get_valid_moves(self.game.board))\n","        if not moves:\n","            return -1 if rollout_player == 1 else 1  # no moves -> lose\n","\n","        action = moves[np.random.randint(len(moves))]\n","        self.game.apply_move(action )  # apply move for correct player\n","\n","        result = self.game.EndGame()\n","        if result != 0:\n","            return result\n","\n","        self.game.board = self.game.get_opponent()  # flip board for next player\n","        rollout_player *= -1\n","\n","    # if max steps exceeded, return draw\n","     return 0\n","\n","    def backpropagate(self, value):\n","        self.value_sum += value\n","        self.visit_count += 1\n","\n","        if self.parent is not None:\n","            self.parent.backpropagate(-value)\n","\n","\n","class MCTS:\n","    def __init__(self, game, args):\n","        self.game = game\n","        self.args = args\n","\n","    def search(self, state):\n","        root = Node(self.game, self.args, state)\n","\n","        for _ in range(self.args[\"num_searches\"]):\n","            node = root\n","\n","            # Selection\n","            while node.is_fully_expanded():\n","                node = node.select()\n","\n","            # Expansion\n","            result = self.game.EndGame()\n","            if result == 0:  # not terminal\n","                node = node.expand()\n","                value = node.simulate()\n","            else:\n","                value = result\n","\n","            # Backpropagation\n","            node.backpropagate(value)\n","\n","        # Convert visit counts to probabilities\n","        action_probs = defaultdict(float)\n","        total_visits = sum(child.visit_count for child in root.children)\n","        for child in root.children:\n","            action_probs[child.action_taken] = child.visit_count / total_visits\n","\n","        return action_probs\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhIqlvr4Rdjv"},"outputs":[],"source":["import torch\n","\n","def mask_illegal_moves(policy, available_moves):\n","\n","\n","    mask = torch.zeros_like(policy[0])\n","\n","    for (i,j), moves in available_moves.items():\n","        for direction, valid in enumerate(moves):\n","            if valid:\n","                index = (i*8 + j)*4 + direction\n","                mask[index] = 1\n","\n","    masked_policy = policy * mask\n","\n","\n","    if masked_policy.sum() > 0:\n","        masked_policy = masked_policy / masked_policy.sum()\n","\n","    return masked_policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pV3BImyGVxFn"},"outputs":[],"source":["def apply_policy_to_state(state, policy, available_moves):\n","    \"\"\"\n","    Apply a policy to a board state to get the next state.\n","    \"\"\"\n","    masked_policy = mask_illegal_moves(policy, available_moves)\n","\n","    masked_policy_np = masked_policy.detach().cpu().numpy().flatten()\n","\n","    # Ensure all probabilities are non-negative\n","    masked_policy_np = np.clip(masked_policy_np, 0, None)\n","\n","    # If sum is zero (no legal moves), return current state\n","    total = masked_policy_np.sum()\n","    if total == 0:\n","        print(\"No legal moves available\")\n","        return state\n","    masked_policy_np /= total  # normalize\n","\n","    # Sample action\n","    action_index = np.random.choice(len(masked_policy_np), p=masked_policy_np)\n","\n","    # Decode action_index to (i, j, direction)\n","    i = action_index // 32\n","    j = (action_index % 32) // 4\n","    direction = action_index % 4\n","    action = ((i, j), direction)\n","\n","    # Apply move (assuming you have a self-contained apply_move)\n","    next_state = state.copy()\n","    next_state = game.apply_move(  action )  # adjust player if needed\n","\n","    return next_state\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdjH9WYNL7mf"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        # Two 3x3 convolutions inside residual block\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out = F.relu(x + out)  # Residual connection\n","        return out\n","\n","class CheckersCNN(nn.Module):\n","    def __init__(self, num_res_blocks=4, board_channels=5, torso_channels=256, policy_size=512):\n","        super().__init__()\n","        # Initial conv layer (like AlphaZero)\n","        self.conv_in = nn.Conv2d(board_channels, torso_channels, kernel_size=3, padding=1)\n","        self.bn_in = nn.BatchNorm2d(torso_channels)\n","\n","        # Residual blocks\n","        self.res_blocks = nn.ModuleList([ResidualBlock(torso_channels) for _ in range(num_res_blocks)])\n","\n","        # Policy head\n","        self.policy_conv = nn.Conv2d(torso_channels, 2, kernel_size=1)  # reduce channels to 2\n","        self.policy_bn = nn.BatchNorm2d(2)\n","        self.policy_fc = nn.Linear(2*8*8, policy_size)  # flatten 8x8x2 → FC\n","        # Policy output will be logits of size `policy_size`\n","\n","        # Value head\n","        self.value_conv = nn.Conv2d(torso_channels, 1, kernel_size=1)  # reduce channels to 1\n","        self.value_bn = nn.BatchNorm2d(1)\n","        self.value_fc1 = nn.Linear(8*8*1, 64)\n","        self.value_fc2 = nn.Linear(64, 1)  # tanh output [-1,1]\n","\n","    def forward(self, x):\n","        # Input conv\n","        x = F.relu(self.bn_in(self.conv_in(x)))\n","\n","        # Residual torso\n","        for block in self.res_blocks:\n","            x = block(x)\n","\n","        # Policy head\n","        p = F.relu(self.policy_bn(self.policy_conv(x)))\n","        p = p.view(p.size(0), -1)\n","        p = self.policy_fc(p)  # raw logits\n","\n","        # Value head\n","        v = F.relu(self.value_bn(self.value_conv(x)))\n","        v = v.view(v.size(0), -1)\n","        v = F.relu(self.value_fc1(v))\n","        v = torch.tanh(self.value_fc2(v))  # output between -1 and 1\n","\n","        return p, v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwDfoMk8zso4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"6RKxURUmf5oI","outputId":"5da9d6b7-cdd4-4ab5-8f95-fad288d65222"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Your possible moves (Black = -1):\n","0: ((5, 0), 1)\n","1: ((5, 2), 0)\n","2: ((5, 2), 1)\n","3: ((5, 4), 0)\n","4: ((5, 4), 1)\n","5: ((5, 6), 0)\n","6: ((5, 6), 1)\n","Select move index: 0\n","\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Agent chooses move: ((2, 5), 1)\n","\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  0  0  1]\n"," [ 0  0  0  0  0  0  1  0]\n"," [ 0 -1  0  0  0  0  0  0]\n"," [ 0  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Your possible moves (Black = -1):\n","0: ((4, 1), 0)\n","1: ((4, 1), 1)\n","2: ((5, 2), 1)\n","3: ((5, 4), 0)\n","4: ((5, 4), 1)\n","5: ((5, 6), 0)\n","6: ((5, 6), 1)\n","7: ((6, 1), 0)\n","Select move index: 0\n","\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  0  0  1]\n"," [-1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Agent chooses move: ((3, 6), 0)\n","\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  0  0  1]\n"," [-1  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  1  0  0]\n"," [ 0  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Your possible moves (Black = -1):\n","0: ((5, 2), 0)\n","1: ((5, 2), 1)\n","2: ((5, 4), 0)\n","3: ((5, 4), 1)\n","4: ((5, 6), 0)\n","5: ((5, 6), 1)\n","6: ((6, 1), 0)\n","Select move index: 4\n","\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  0  0  1]\n"," [-1  0  0  0 -1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -1  0 -1  0  0  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Agent chooses move: ((2, 7), 0)\n","\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  0  0  0]\n"," [-1  0  0  0 -1  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -1  0 -1  0  0  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Your possible moves (Black = -1):\n","0: ((3, 4), 1)\n","1: ((5, 2), 0)\n","2: ((5, 2), 1)\n","3: ((5, 4), 0)\n","4: ((5, 4), 1)\n","5: ((6, 1), 0)\n","6: ((6, 5), 1)\n","7: ((6, 7), 0)\n","Select move index: 7\n","\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  0  0  0]\n"," [-1  0  0  0 -1  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0  0]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Agent chooses move: ((2, 3), 1)\n","\n","Current Board:\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [-1  0  0  0  0  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0 -1  0 -1  0  0  0]\n"," [ 0 -1  0 -1  0 -1  0  1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","\n","Your possible moves (Black = -1):\n","0: ((5, 2), 0)\n","1: ((5, 2), 1)\n","2: ((5, 4), 0)\n","3: ((5, 4), 1)\n","4: ((6, 1), 0)\n","5: ((6, 5), 1)\n"]}],"source":["import torch\n","import numpy as np\n","from collections import defaultdict\n","\n","# --------------------------\n","# Board to tensor function\n","# --------------------------\n","def board_to_tensor(board, current_player=1):\n","    planes = np.zeros((5, 8, 8), dtype=np.float32)\n","    planes[0] = (board == 1).astype(np.float32)   # White pieces\n","    planes[1] = (board == 2).astype(np.float32)   # White kings\n","    planes[2] = (board == -1).astype(np.float32)  # Black pieces\n","    planes[3] = (board == -2).astype(np.float32)  # Black kings\n","    planes[4] = np.ones((8, 8), dtype=np.float32) * current_player\n","    return planes\n","def Possible_cap_b(board, i, j, player):\n","    \"\"\"Return possible capture moves for a piece at (i,j)\"\"\"\n","    d = defaultdict(lambda: np.zeros(4))\n","    n = 8\n","\n","    if player == -1:  # Black pieces\n","        if board[i][j] <= 0:\n","            # Forward left\n","            if i-2 >= 0 and j-2 >= 0 and board[i-2][j-2] == 0:\n","                if board[i-1][j-1] != 0 and board[i-1][j-1] >= 1:\n","                    d[i, j][0] = 1\n","            # Forward right\n","            if i-2 >= 0 and j+2 < n and board[i-2][j+2] == 0:\n","                if board[i-1][j+1] != 0 and board[i-1][j+1] >= 1:\n","                    d[i, j][1] = 1\n","        if board[i][j] < 0:  # King moves\n","            # Backward left\n","            if i+2 < n and j-2 >= 0 and board[i+2][j-2] == 0:\n","                if board[i+1][j-1] != 0 and board[i+1][j-1] >= 1:\n","                    d[i, j][2] = 1\n","            # Backward right\n","            if i+2 < n and j+2 < n and board[i+2][j+2] == 0:\n","                if board[i+1][j+1] != 0 and board[i+1][j+1] >= 1:\n","                    d[i, j][3] = 1\n","\n","    if player == 1:  # White pieces\n","        if board[i][j] >= 1:\n","            # Forward left\n","            if i+2 < n and j-2 >= 0 and board[i+2][j-2] == 0:\n","                if board[i+1][j-1] != 0 and board[i+1][j-1] <= 0:\n","                    d[i, j][0] = 1\n","            # Forward right\n","            if i+2 < n and j+2 < n and board[i+2][j+2] == 0:\n","                if board[i+1][j+1] != 0 and board[i+1][j+1] <= 0:\n","                    d[i, j][1] = 1\n","        if board[i][j] > 1:  # King moves\n","            # Backward left\n","            if i-2 >= 0 and j-2 >= 0 and board[i-2][j-2] == 0:\n","                if board[i-1][j-1] != 0 and board[i-1][j-1] <= 0:\n","                    d[i, j][2] = 1\n","            # Backward right\n","            if i-2 >= 0 and j+2 < n and board[i-2][j+2] == 0:\n","                if board[i-1][j+1] != 0 and board[i-1][j+1] <= 0:\n","                    d[i, j][3] = 1\n","\n","    # Convert d dictionary to move list\n","    move_list = []\n","    for pos, arr in d.items():\n","        for idx, val in enumerate(arr):\n","            if val == 1:\n","                move_list.append((pos, idx))\n","    return move_list\n","\n","\n","\n","def apply_move_b(board, action, player):\n","    i, j = action[0]\n","    move = action[1]\n","    n = 8  # board size\n","\n","    # Forward-left\n","    if move == 0:\n","        if board[i-1][j-1] == 0:\n","            board[i-1][j-1] = board[i][j]\n","            board[i][j] = 0\n","            if i-1 == 0 and board[i-1][j-1] == -1:\n","                board[i-1][j-1] -= 1  # Promote to king\n","        else:\n","            captured_piece = board[i-1][j-1]\n","            board[i-1][j-1] = 0\n","            board[i-2][j-2] = board[i][j]\n","            board[i][j] = 0\n","            if i-2 == 0 and board[i-2][j-2] == -1:\n","                board[i-2][j-2] -= 1\n","            act = Possible_cap_b(board, i-2, j-2, player)\n","            if act:\n","                for mv in act:\n","                    return apply_move_b(board, mv, player)\n","\n","    # Forward-right\n","    elif move == 1:\n","        if board[i-1][j+1] == 0:\n","            board[i-1][j+1] = board[i][j]\n","            board[i][j] = 0\n","            if i-1 == 0 and board[i-1][j+1] == -1:\n","                board[i-1][j+1] -= 1\n","        else:\n","            captured_piece = board[i-1][j+1]\n","            board[i-1][j+1] = 0\n","            board[i-2][j+2] = board[i][j]\n","            board[i][j] = 0\n","            if i-2 == 0 and board[i-2][j+2] == -1:\n","                board[i-2][j+2] -= 1\n","            act = Possible_cap_b(board, i-2, j+2, player)\n","            if act:\n","                for mv in act:\n","                    return apply_move_b(board, mv, player)\n","\n","    # Backward-left\n","    elif move == 2:\n","        if board[i+1][j-1] == 0:\n","            board[i+1][j-1] = board[i][j]\n","            board[i][j] = 0\n","        else:\n","            captured_piece = board[i+1][j-1]\n","            board[i+1][j-1] = 0\n","            board[i+2][j-2] = board[i][j]\n","            board[i][j] = 0\n","            act = Possible_cap_b(board, i+2, j-2, player)\n","            if act:\n","                for mv in act:\n","                    return apply_move_b(board, mv, player)\n","\n","    # Backward-right\n","    elif move == 3:\n","        if board[i+1][j+1] == 0:\n","            board[i+1][j+1] = board[i][j]\n","            board[i][j] = 0\n","        else:\n","            captured_piece = board[i+1][j+1]\n","            board[i+1][j+1] = 0\n","            board[i+2][j+2] = board[i][j]\n","            board[i][j] = 0\n","            act = Possible_cap_b(board, i+2, j+2, player)\n","            if act:\n","                for mv in act:\n","                    return apply_move_b(board, mv, player)\n","\n","    return board\n","\n","def play_against_agent(model, device=\"cuda\"):\n","    game = FixedCheckers()\n","    game.current_player = -1  # Human starts at top\n","\n","    while True:\n","        print(\"\\nCurrent Board:\")\n","        print(game.board)\n","\n","        # Check if game ended\n","        result = game.EndGame()\n","        if result != 0:\n","            if result == 1:\n","                print(\"White (Agent) wins!\")\n","            elif result == -1:\n","                print(\"Black (Human) wins!\")\n","            else:\n","                print(\"Draw / No moves left!\")\n","            break\n","\n","        # Human turn (-1)\n","        if game.current_player == -1:\n","            moves = game.get_moves(game.get_valid_moves())\n","            if not moves:\n","                print(\"No moves available for Human! You lose.\")\n","                break\n","\n","            print(\"\\nYour possible moves (Black = -1):\")\n","            for idx, mv in enumerate(moves):\n","                print(f\"{idx}: {mv}\")\n","\n","            try:\n","                choice = int(input(\"Select move index: \"))\n","                action = moves[choice]\n","            except:\n","                print(\"Invalid input! Picking first available move.\")\n","                action = moves[0]\n","\n","            apply_move_b(game.board, action,-1)\n","            game.switch_player()\n","\n","        # Agent turn (1)\n","        else:\n","            moves = game.get_moves(game.get_valid_moves())\n","            if not moves:\n","                print(\"No moves available for Agent! You win.\")\n","                break\n","\n","            state = game.board_to_tensor()\n","            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n","            with torch.no_grad():\n","                policy, value = model(state)\n","\n","            policy = torch.softmax(policy[0], dim=0).cpu().numpy()\n","\n","            # Pick the best move from policy\n","            move_idx = np.argmax(policy[:len(moves)])\n","            action = moves[move_idx]\n","\n","            print(\"\\nAgent chooses move:\", action)\n","            game.apply_move(action)\n","            game.switch_player()\n","\n","\n","# --------------------------\n","# Usage\n","# --------------------------\n","if __name__ == \"__main__\":\n","    # Import your CNN model definition here\n","    # from your_model_file import CheckersCNN, FixedCheckers\n","    model = CheckersCNN()  # your model class\n","    device = torch.device(\"cuda\")\n","    model.load_state_dict(torch.load(\"/content/checkers_model_300.pth\", map_location=device))\n","    model.to(device)\n","    model.eval()\n","\n","    play_against_agent(model, device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":543},"executionInfo":{"elapsed":17,"status":"error","timestamp":1758863775085,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"FUCI9Xkl-nZN","outputId":"c1ac6078-42a7-41be-d5cd-8ec947dbbdf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial board:\n","[[ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  2]\n"," [ 0  0 -1  0  0  0 -1  0]\n"," [ 0 -1  0  0  0  0  0  0]\n"," [-1  0  0  0  1  0  1  0]\n"," [ 0  0  0  0  0 -1  0 -1]\n"," [ 0  0  2  0 -1  0  0  0]]\n","defaultdict(<function FixedCheckers.available.<locals>.<lambda> at 0x7f73664677e0>, {(2, 7): array([1., 0., 1., 0.]), (5, 4): array([1., 1., 0., 0.]), (7, 2): array([0., 0., 1., 1.])})\n","((5, 6), 1)\n"]},{"ename":"IndexError","evalue":"index 8 is out of bounds for axis 0 with size 8","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2159763294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mnew_board\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBoard after move:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2895321143.py\u001b[0m in \u001b[0;36mapply_move\u001b[0;34m(self, action, player)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for axis 0 with size 8"]}],"source":["import numpy as np\n","from collections import defaultdict\n","\n","# Assuming FixedCheckers class is already defined above\n","\n","# Your test board\n","B = np.array([[ 0, 0,  0 , 0 , 0,  0 , 0 , 0],\n","             [ 0 ,0 , 0 ,  0,  0 , 0 , 0 , 0],\n","             [ 0 , 0 , 0,  0 ,  0 , 0,  0 , 2],\n","             [ 0  ,0 , -1 , 0,  0,  0 ,-1,  0],\n","             [ 0 ,-1,  0 , 0 , 0 , 0,  0 , 0],\n","             [-1 , 0 , 0,  0,  1,  0 , 1 , 0],\n","             [ 0 , 0,  0 , 0,  0, -1 , 0 , -1],\n","             [ 0 , 0 ,2 , 0, -1 , 0 , 0,  0]])\n","\n","# Initialize game\n","game = FixedCheckers()\n","game.board = B.copy()\n","game.current_player = 1  # White to move\n","\n","print(\"Initial board:\")\n","print(game.board)\n","\n","print(game.available(1 ))\n","new_board = game.apply_move(((5,6),1))\n","\n","print(\"\\nBoard after move:\")\n","print(new_board)\n","\n","# Check for further captures (double jumps)\n","caps = game.Possible_cap(4,5, game.current_player)  # final pos after first capture\n","print(\"\\nPossible additional captures from (4,5):\", caps)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"e_5ZtijhL7j2"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import math\n","import random\n","import matplotlib.pyplot as plt\n","from collections import defaultdict, deque\n","from tqdm import tqdm\n","\n","# Fix the missing board_to_tensor function\n","def board_to_tensor(board, current_player=1):\n","    \"\"\"Convert board to 5-plane tensor representation\"\"\"\n","    planes = np.zeros((5, 8, 8), dtype=np.float32)\n","    planes[0] = (board == 1).astype(np.float32)   # White pieces\n","    planes[1] = (board == 2).astype(np.float32)   # White kings\n","    planes[2] = (board == -1).astype(np.float32)  # Black pieces\n","    planes[3] = (board == -2).astype(np.float32)  # Black kings\n","    planes[4] = np.ones((8, 8), dtype=np.float32) * current_player  # Current player\n","    return planes\n","\n","class FixedCheckers:\n","    \"\"\"Fixed version of your Checkers class\"\"\"\n","    def __init__(self):\n","        self.board = np.array([\n","            [0,   1, 0,   1, 0,  1, 0,  1],\n","            [ 1,  0,  1,  0,  1,  0, 1,  0],\n","            [0,  1, 0,  1, 0,  1, 0,   1],\n","            [0,  0, 0,  0, 0,  0, 0,  0],\n","            [0,  0, 0,  0, 0,  0, 0,  0],\n","            [-1, 0,  -1, 0,  -1, 0,  -1, 0],\n","            [0,  -1, 0,  -1, 0,  -1, 0,  -1],\n","            [ -1, 0,  -1, 0, -1, 0,  -1, 0]\n","        ], dtype=int)\n","        self.current_player = 1\n","\n","    def board_to_tensor(self):\n","        \"\"\"Always return board from current player's perspective\"\"\"\n","        if self.current_player == 1:\n","            return board_to_tensor(self.board, 1)\n","        else:\n","            # Flip board so black player sees it from their perspective\n","            flipped_board = np.flipud(np.fliplr(-self.board))\n","            return board_to_tensor(flipped_board, 1)\n","\n","    def copy(self):\n","        \"\"\"Create a copy of the game state\"\"\"\n","        new_game = FixedCheckers()\n","        new_game.board = self.board.copy()\n","        new_game.current_player = self.current_player\n","        return new_game\n","\n","    def get_valid_moves(self, player=None):\n","        \"\"\"Get valid moves for current player\"\"\"\n","        if player is None:\n","            player = self.current_player\n","        return self.available(player)\n","\n","    def available(self, player=1):\n","        d = defaultdict(lambda: np.zeros(4))\n","\n","\n","\n","        if player == 1:  # white pieces\n","            for i in range(len(self.board)):\n","                for j in range(len(self.board)):\n","                    if self.board[i][j] != 0 and self.board[i][j] >= player:\n","                        # Forward left\n","                        if i+1 < 8 and j-1 >= 0:\n","                            if self.board[i+1][j-1] == 0:\n","                                d[i, j][0] = 1\n","                            elif self.board[i+1][j-1] <  0:\n","                                if i+2 < 8 and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                                    d[i, j][0] = 1\n","\n","                        if i+1 < 8 and j+1 < 8:\n","                            if self.board[i+1][j+1] == 0:\n","                                d[i, j][1] = 1\n","                            elif self.board[i+1][j+1] <  0:\n","                                if i+2 < 8 and j+2 < = 7 and self.board[i+2][j+2] == 0:\n","                                    d[i,j][1] = 1\n","\n","                    if self.board[i][j] != 0 and self.board[i][j] > player:\n","                        # Backward left\n","                        if i-1 >= 0 and j-1 >= 0:\n","                            if self.board[i-1][j-1] == 0:\n","                                d[i, j][2] = 1\n","                            elif self.board[i-1][j-1]  < 0:\n","                                if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                                    d[i, j][2] = 1\n","                        # Backward right\n","                        if i-1 >= 0 and j+1 < 8:\n","                            if self.board[i-1][j+1] == 0:\n","                                d[i, j][3] = 1\n","                            elif self.board[i-1][j+1]  < 0:\n","                                if i-2 >= 0 and j+2 < 8 and self.board[i-2][j+2] == 0:\n","                                    d[i, j][3] = 1\n","        return d\n","\n","\n","    def apply_move(self, action, player=1):\n","     print(action)\n","     i, j = action[0]\n","     move = action[1]\n","\n","     if player == 1:  # White player\n","        if move == 0:  # forward-left\n","            if self.board[i+1][j-1] == 0:\n","                self.board[i+1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j-1] == 1:\n","                    self.board[i+1][j-1] += 1\n","\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j-1]\n","\n","                self.board[i+1][j-1] = 0\n","                self.board[i+2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j-2] == 1:\n","                    self.board[i+2][j-2] += 1\n","\n","                act = self.Possible_cap(i+2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 1:  # forward-right\n","            if self.board[i+1][j+1] == 0:\n","                self.board[i+1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j+1] == 1:\n","                    self.board[i+1][j+1] += 1\n","                    # Extra point for king promotion\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j+1]\n","                   # Remove from black's score\n","\n","                self.board[i+1][j+1] = 0\n","                self.board[i+2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j+2] == 1:\n","                    self.board[i+2][j+2] += 1\n","\n","                act = self.Possible_cap(i+2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 2:  # backward-left\n","            if self.board[i-1][j-1] == 0:\n","                self.board[i-1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j-1]\n","\n","\n","                self.board[i-1][j-1] = 0\n","                self.board[i-2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 3:  # backward-right\n","            if self.board[i-1][j+1] == 0:\n","                self.board[i-1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j+1]\n","\n","\n","                self.board[i-1][j+1] = 0\n","                self.board[i-2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","     return self.board\n","\n","    def Possible_cap(self, i, j, player=1):\n","     d   = defaultdict(lambda: np.zeros(4))\n","     n = 8\n","\n","     if self.board[i][j] >= 1:\n","            # Forward left\n","            if i+2 < n and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                if self.board[i+1][j-1] != 0 and self.board[i+1][j-1] <= 0:\n","                    d[i, j][0] = 1\n","            # Forward right\n","            if i+2 < n and j+2 < n and self.board[i+2][j+2] == 0:\n","                if self.board[i+1][j+1] != 0 and self.board[i+1][j+1] <= 0:\n","                    d[i, j][1] = 1\n","     if self.board[i][j] > 1:\n","\n","            if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                if self.board[i-1][j-1] != 0 and self.board[i-1][j-1] <= 0:\n","                    d[i, j][2] = 1\n","            # Backward right\n","            if i-2 >= 0 and j+2 < n and self.board[i-2][j+2] == 0:\n","                if self.board[i-1][j+1] != 0 and self.board[i-1][j+1] <= 0:\n","                    d[i, j][3] = 1\n","\n","     return self.get_moves(d)\n","\n","\n","\n","\n","\n","\n","     return self.get_moves(d)\n","\n","    def get_moves(self, d):\n","        \"\"\"Convert move dict to list\"\"\"\n","        move_list = []\n","        for pos, arr in d.items():\n","            for i, val in enumerate(arr):\n","                if val == 1:\n","                    move_list.append((pos, i))\n","        return move_list\n","\n","    def switch_player(self):\n","        \"\"\"Explicitly switch the current player\"\"\"\n","        self.current_player *= -1\n","\n","    def EndGame(self):\n","        \"\"\"Check if game has ended\"\"\"\n","        white_pieces = np.sum(self.board > 0)\n","        black_pieces = np.sum(self.board < 0)\n","\n","        if white_pieces == 0:\n","            return -1  # Black wins\n","        elif black_pieces == 0:\n","            return 1   # White wins\n","\n","        # Check if current player has moves\n","        moves = self.get_moves(self.get_valid_moves(self.current_player))\n","        if not moves:\n","            return -self.current_player  # Current player loses\n","\n","        return 0  # Game continues\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out = F.relu(x + out)\n","        return out\n","\n","class CheckersCNN(nn.Module):\n","    def __init__(self, num_res_blocks=4, board_channels=5, torso_channels=128):\n","        super().__init__()\n","        # Initial conv layer\n","        self.conv_in = nn.Conv2d(board_channels, torso_channels, kernel_size=3, padding=1)\n","        self.bn_in = nn.BatchNorm2d(torso_channels)\n","\n","        # Residual blocks\n","        self.res_blocks = nn.ModuleList([ResidualBlock(torso_channels) for _ in range(num_res_blocks)])\n","\n","        # Policy head (256 possible moves: 64 squares * 4 directions)\n","        self.policy_conv = nn.Conv2d(torso_channels, 2, kernel_size=1)\n","        self.policy_bn = nn.BatchNorm2d(2)\n","        self.policy_fc = nn.Linear(2*8*8, 256)\n","\n","        # Value head\n","        self.value_conv = nn.Conv2d(torso_channels, 1, kernel_size=1)\n","        self.value_bn = nn.BatchNorm2d(1)\n","        self.value_fc1 = nn.Linear(8*8, 64)\n","        self.value_fc2 = nn.Linear(64, 1)\n","\n","    def forward(self, x):\n","        # Input conv\n","        x = F.relu(self.bn_in(self.conv_in(x)))\n","\n","        # Residual torso\n","        for block in self.res_blocks:\n","            x = block(x)\n","\n","        # Policy head\n","        p = F.relu(self.policy_bn(self.policy_conv(x)))\n","        p = p.view(p.size(0), -1)\n","        p = self.policy_fc(p)\n","\n","        # Value head\n","        v = F.relu(self.value_bn(self.value_conv(x)))\n","        v = v.view(v.size(0), -1)\n","        v = F.relu(self.value_fc1(v))\n","        v = torch.tanh(self.value_fc2(v))\n","\n","        return p, v\n","\n","class Node:\n","    def __init__(self, game_state, parent=None, action_taken=None, prior=0):\n","        self.game_state = game_state.copy()\n","        self.parent = parent\n","        self.action_taken = action_taken\n","        self.prior = prior\n","\n","        self.children = {}\n","        self.visit_count = 0\n","        self.value_sum = 0\n","        self.is_expanded = False\n","\n","    def is_terminal(self):\n","        return self.game_state.EndGame() != 0\n","\n","    def expand(self, policy):\n","        \"\"\"Expand node with policy from neural network\"\"\"\n","        if self.is_expanded:\n","            return\n","\n","        moves_dict = self.game_state.get_valid_moves()\n","        moves = self.game_state.get_moves(moves_dict)\n","\n","        if not moves:\n","            self.is_expanded = True\n","            return\n","\n","        # Convert moves to policy indices\n","        policy_probs = F.softmax(policy, dim=0)\n","\n","        for move in moves:\n","            (i, j), direction = move\n","            policy_idx = i * 32 + j * 4 + direction\n","            prior = policy_probs[policy_idx].item()\n","\n","            new_game = self.game_state.copy()\n","            new_game.apply_move(move)\n","            new_game.switch_player()  # Explicitly switch player\n","\n","            child = Node(new_game, parent=self, action_taken=move, prior=prior)\n","            self.children[move] = child\n","\n","        self.is_expanded = True\n","\n","    def select_child(self, c_puct=1.0):\n","        \"\"\"Select child with highest UCB score\"\"\"\n","        best_score = -float('inf')\n","        best_child = None\n","\n","        for child in self.children.values():\n","            q_value = 0 if child.visit_count == 0 else child.value_sum / child.visit_count\n","            u_value = c_puct * child.prior * math.sqrt(self.visit_count) / (1 + child.visit_count)\n","            score = q_value + u_value\n","\n","            if score > best_score:\n","                best_score = score\n","                best_child = child\n","\n","        return best_child\n","\n","    def backup(self, value):\n","        \"\"\"Backup value through the tree\"\"\"\n","        self.visit_count += 1\n","        self.value_sum += value\n","\n","        if self.parent:\n","            self.parent.backup(-value)\n","\n","class MCTS:\n","    def __init__(self, model, device, c_puct=1.0):\n","        self.model = model\n","        self.device = device\n","        self.c_puct = c_puct\n","\n","    def search(self, game_state, num_simulations=100):\n","        \"\"\"Run MCTS search\"\"\"\n","        root = Node(game_state)\n","\n","        for _ in range(num_simulations):\n","            node = root\n","\n","            # Selection\n","            while node.is_expanded and not node.is_terminal():\n","                node = node.select_child(self.c_puct)\n","\n","            # Evaluation\n","            if node.is_terminal():\n","                value = node.game_state.EndGame()\n","            else:\n","                # Get neural network prediction\n","                board_tensor = torch.tensor(node.game_state.board_to_tensor(), dtype=torch.float32).unsqueeze(0).to(self.device)\n","                with torch.no_grad():\n","                    policy, value = self.model(board_tensor)\n","                    value = value.item()\n","\n","                # Expansion\n","                node.expand(policy.squeeze(0))\n","\n","            # Backup\n","            node.backup(value)\n","\n","        # Return visit count distribution as policy\n","        moves_dict = game_state.get_valid_moves()\n","        moves = game_state.get_moves(moves_dict)\n","\n","        policy = np.zeros(256)\n","        for move in moves:\n","            if move in root.children:\n","                (i, j), direction = move\n","                policy_idx = i * 32 + j * 4 + direction\n","                policy[policy_idx] = root.children[move].visit_count\n","\n","        if policy.sum() > 0:\n","            policy = policy / policy.sum()\n","\n","        return policy, root.value_sum / max(root.visit_count, 1)\n","\n","class AlphaZeroTrainer:\n","    def __init__(self, model, lr=0.001):\n","        self.model = model\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.model.to(self.device)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n","        self.mcts = MCTS(self.model, self.device)\n","\n","        self.training_data = deque(maxlen=10000)\n","\n","    def self_play(self, num_simulations=25):\n","        \"\"\"Play one self-play game with consistent perspective\"\"\"\n","        game = FixedCheckers()\n","        training_examples = []\n","        move_count = 0\n","\n","        while move_count < 200:  # Prevent infinite games\n","            move_count += 1\n","\n","            # Get MCTS policy (always from current player's perspective)\n","            policy, _ = self.mcts.search(game, num_simulations)\n","\n","            # Store training example (always from player 1's perspective due to board_to_tensor)\n","            board_tensor = game.board_to_tensor()\n","            training_examples.append((board_tensor, policy, None, game.current_player))\n","\n","            # Sample action from policy\n","            valid_indices = []\n","            valid_probs = []\n","            moves_dict = game.get_valid_moves()\n","            moves = game.get_moves(moves_dict)\n","\n","            for move in moves:\n","                (i, j), direction = move\n","                idx = i * 32 + j * 4 + direction\n","                if policy[idx] > 0:\n","                    valid_indices.append(idx)\n","                    valid_probs.append(policy[idx])\n","\n","            if not valid_indices:\n","                break\n","\n","            valid_probs = np.array(valid_probs)\n","            valid_probs = valid_probs / valid_probs.sum()\n","\n","            chosen_idx = np.random.choice(len(valid_indices), p=valid_probs)\n","            policy_idx = valid_indices[chosen_idx]\n","\n","            # Convert back to move\n","            i = policy_idx // 32\n","            j = (policy_idx % 32) // 4\n","            direction = policy_idx % 4\n","            action = ((i, j), direction)\n","            # Apply move and switch players\n","            game.apply_move(action)\n","            game.switch_player()\n","\n","            # Check if game ended\n","            result = game.EndGame()\n","            if result != 0:\n","                # Fill in values for training examples\n","                for idx, (board, pol, _, player_who_moved) in enumerate(training_examples):\n","                    # Value from perspective of the player who made the move\n","                    if player_who_moved == result:\n","                        value = 1.0  # Win\n","                    elif player_who_moved == -result:\n","                        value = -1.0  # Loss\n","                    else:\n","                        value = 0.0  # Draw\n","\n","                    training_examples[idx] = (board, pol, value)\n","\n","                break\n","\n","        # Filter out the player information and return clean training examples\n","        clean_examples = []\n","        for example in training_examples:\n","            if len(example) == 3:  # Already processed (board, policy, value)\n","                board, policy, value = example\n","                if value is not None:\n","                    clean_examples.append((board, policy, value))\n","            elif len(example) == 4:  # Not yet processed (board, policy, None, player)\n","                # This shouldn't happen if the loop above worked correctly\n","                continue\n","\n","        return clean_examples\n","\n","    def train_step(self, batch_size=32):\n","        \"\"\"Train the model on a batch of data\"\"\"\n","        if len(self.training_data) < batch_size:\n","            return 0, 0\n","\n","        # Sample batch\n","        batch = random.sample(list(self.training_data), batch_size)\n","\n","        boards = torch.tensor([x[0] for x in batch], dtype=torch.float32).to(self.device)\n","        target_policies = torch.tensor([x[1] for x in batch], dtype=torch.float32).to(self.device)\n","        target_values = torch.tensor([[x[2]] for x in batch], dtype=torch.float32).to(self.device)\n","\n","        # Forward pass\n","        pred_policies, pred_values = self.model(boards)\n","\n","        # Calculate losses\n","        policy_loss = F.cross_entropy(pred_policies, target_policies)\n","        value_loss = F.mse_loss(pred_values, target_values)\n","        total_loss = policy_loss + value_loss\n","\n","        # Backward pass\n","        self.optimizer.zero_grad()\n","        total_loss.backward()\n","        self.optimizer.step()\n","\n","        return policy_loss.item(), value_loss.item()\n","\n","    def train(self, iterations=1000, games_per_iteration=10, train_steps_per_iteration=10):\n","        \"\"\"Main training loop\"\"\"\n","        print(f\"Training on device: {self.device}\")\n","\n","        policy_losses = []\n","        value_losses = []\n","\n","        for iteration in tqdm(range(iterations)):\n","            # Self-play games\n","            for _ in range(games_per_iteration):\n","                examples = self.self_play()\n","                self.training_data.extend(examples)\n","\n","            # Training steps\n","            total_policy_loss = 0\n","            total_value_loss = 0\n","\n","            for _ in range(train_steps_per_iteration):\n","                p_loss, v_loss = self.train_step()\n","                total_policy_loss += p_loss\n","                total_value_loss += v_loss\n","\n","            avg_policy_loss = total_policy_loss / train_steps_per_iteration\n","            avg_value_loss = total_value_loss / train_steps_per_iteration\n","\n","            policy_losses.append(avg_policy_loss)\n","            value_losses.append(avg_value_loss)\n","\n","            if iteration % 1 == 0:\n","                print(f\"Iteration {iteration}: Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}\")\n","                print(f\"Training data size: {len(self.training_data)}\")\n","\n","                # Save checkpoint\n","                torch.save(self.model.state_dict(), f'checkers_model_{iteration}.pth')\n","\n","        # Plot training progress\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","        ax1.plot(policy_losses)\n","        ax1.set_title('Policy Loss')\n","        ax1.set_xlabel('Iteration')\n","        ax1.set_ylabel('Loss')\n","\n","        ax2.plot(value_losses)\n","        ax2.set_title('Value Loss')\n","        ax2.set_xlabel('Iteration')\n","        ax2.set_ylabel('Loss')\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Save final model\n","        torch.save(self.model.state_dict(), 'checkers_final_model.pth')\n","\n","        return policy_losses, value_losses\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":877},"executionInfo":{"elapsed":7196,"status":"error","timestamp":1758992360264,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"4ceIkT_ISnzr","outputId":"e518bb30-4acc-4c58-9964-d4d44d8abf9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training on device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/5 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"IndexError","evalue":"Dimension out of range (expected to be in range of [-1, 0], but got 1)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2807278722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     policy_losses, value_losses = trainer.train(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mgames_per_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4230814525.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, iterations, games_per_iteration, train_steps_per_iteration)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames_per_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;31m# Anneal temperature: exploration in first 30 moves, greedy later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_play_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4230814525.py\u001b[0m in \u001b[0;36mself_play_game\u001b[0;34m(self, num_simulations, temperature)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Run MCTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mmcts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_simulations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Get valid moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3603302327.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, game_state, num_simulations)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# -------- Expansion --------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# -------- Backup --------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-769689915.py\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, policy_logits)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# 3. Apply softmax on logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m      \u001b[0mpolicy_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# 4. Mask invalid moves and renormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2137\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"]}],"source":["# Usage\n","if __name__ == \"__main__\":\n","    # Create model\n","    model = CheckersCNN()\n","\n","    # Create trainer\n","    trainer = AlphaZeroTrainer(model, lr=0.001)\n","\n","    # Train\n","    policy_losses, value_losses = trainer.train(\n","        iterations=5,\n","        games_per_iteration=10,\n","        train_steps_per_iteration=10\n","    )\n","\n","    print(\"Training completed!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_M9ZMHFwlPzL"},"outputs":[],"source":["import numpy as np\n","import random\n","\n","class Checkers:\n","    def __init__(self):\n","        self.board = np.zeros((8, 8), dtype=int)\n","\n","        # Place Black (-1) at TOP\n","        for i in range(3):\n","            for j in range(8):\n","                if (i + j) % 2 == 1:\n","                    self.board[i, j] = -1\n","\n","        # Place White (+1) at BOTTOM\n","        for i in range(5, 8):\n","            for j in range(8):\n","                if (i + j) % 2 == 1:\n","                    self.board[i, j] = +1\n","\n","        self.current_player = -1  # Human starts\n","\n","        # Directions: Black goes down, White goes up\n","        self.DIRECTIONS = {\n","            -1: {0: (1, -1), 1: (1, +1)},   # Human (Black)\n","            +1: {0: (-1, -1), 1: (-1, +1)}  # Agent (White)\n","        }\n","\n","    def get_legal_moves(self, player):\n","        moves = []\n","        for i in range(8):\n","            for j in range(8):\n","                if self.board[i, j] == player:\n","                    for d, (di, dj) in self.DIRECTIONS[player].items():\n","                        ni, nj = i + di, j + dj\n","                        if 0 <= ni < 8 and 0 <= nj < 8 and self.board[ni, nj] == 0:\n","                            moves.append(((i, j), d))\n","        return moves\n","    def Possible_cap(self,i, j , player):\n","\n","     d = defaultdict(lambda: np.zeros(4))\n","     n = 8\n","\n","     if player == -1:\n","        if self.board[i][j] <= 0:\n","            # Forward left\n","            if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                if self.board[i-1][j-1] != 0 and self.board[i-1][j-1] >= 1:\n","                    d[i, j][0] = 1\n","            # Forward right\n","            if i-2 >= 0 and j+2 < n and self.board[i-2][j+2] == 0:\n","                if self.board[i-1][j+1] != 0 and self.board[i-1][j+1] >= 1:\n","                    d[i, j][1] = 1\n","        if self.board[i][j] < 0:\n","            # Backward left\n","            if i+2 < n and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                if self.board[i+1][j-1] != 0 and self.board[i+1][j-1] >= 1:\n","                    d[i, j][2] = 1\n","            # Backward right\n","            if i+2 < n and j+2 < n and self.board[i+2][j+2] == 0:\n","                if self.board[i+1][j+1] != 0 and self.board[i+1][j+1] >= 1:\n","                    d[i, j][3] = 1\n","\n","     if player == 1:  # white pieces\n","        if self.board[i][j] >= 1:\n","            # Forward left\n","            if i+2 < n and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                if self.board[i+1][j-1] != 0 and self.board[i+1][j-1] <= 0:\n","                    d[i, j][0] = 1\n","            # Forward right\n","            if i+2 < n and j+2 < n and self.board[i+2][j+2] == 0:\n","                if self.board[i+1][j+1] != 0 and self.board[i+1][j+1] <= 0:\n","                    d[i, j][1] = 1\n","        if self.board[i][j] > 1:\n","\n","            if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                if self.board[i-1][j-1] != 0 and self.board[i-1][j-1] <= 0:\n","                    d[i, j][2] = 1\n","            # Backward right\n","            if i-2 >= 0 and j+2 < n and self.board[i-2][j+2] == 0:\n","                if self.board[i-1][j+1] != 0 and self.board[i-1][j+1] <= 0:\n","                    d[i, j][3] = 1\n","\n","     return self.get_moves(d)\n","\n","       def apply_move(self, action, player):\n","     i, j = action[0]\n","     move = action[1]\n","\n","     if player >= 1:  # White player\n","        if move == 0:  # forward-left\n","            if self.board[i+1][j-1] == 0:\n","                self.board[i+1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j-1] == 1:\n","                    self.board[i+1][j-1] += 1\n","                    self.score1 += 1  # Extra point for king promotion\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j-1]\n","                self.score2 -= abs(captured_piece)  # Remove from black's score\n","\n","                self.board[i+1][j-1] = 0\n","                self.board[i+2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j-2] == 1:\n","                    self.board[i+2][j-2] += 1\n","                    self.score1 += 1  # Extra point for king promotion\n","                act = self.Possible_cap(i+2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 1:  # forward-right\n","            if self.board[i+1][j+1] == 0:\n","                self.board[i+1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+1 == 7 and self.board[i+1][j+1] == 1:\n","                    self.board[i+1][j+1] += 1\n","                    self.score1 += 1  # Extra point for king promotion\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j+1]\n","                self.score2 -= abs(captured_piece)  # Remove from black's score\n","\n","                self.board[i+1][j+1] = 0\n","                self.board[i+2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i+2 == 7 and self.board[i+2][j+2] == 1:\n","                    self.board[i+2][j+2] += 1\n","                    self.score1 += 1  # Extra point for king promotion\n","                act = self.Possible_cap(i+2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 2:  # backward-left\n","            if self.board[i-1][j-1] == 0:\n","                self.board[i-1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j-1]\n","                self.score2 -= abs(captured_piece)  # Remove from black's score\n","\n","                self.board[i-1][j-1] = 0\n","                self.board[i-2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 3:  # backward-right\n","            if self.board[i-1][j+1] == 0:\n","                self.board[i-1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j+1]\n","                self.score2 -= abs(captured_piece)  # Remove from black's score\n","\n","                self.board[i-1][j+1] = 0\n","                self.board[i-2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i-2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","     elif player <= -1:  # Black player\n","        if move == 0:  # forward-left\n","            if self.board[i-1][j-1] == 0:\n","                self.board[i-1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i-1 == 0 and self.board[i-1][j-1] == -1:\n","                    self.board[i-1][j-1] -= 1\n","                    self.score2 += 1  # Extra point for king promotion\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j-1]\n","                self.score1 -= abs(captured_piece)  # Remove from white's score\n","\n","                self.board[i-1][j-1] = 0\n","                self.board[i-2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i-2 == 0 and self.board[i-2][j-2] == -1:\n","                    self.board[i-2][j-2] -= 1\n","                    self.score2 += 1  # Extra point for king promotion\n","                act = self.Possible_cap(i-2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 1:  # forward-right\n","            if self.board[i-1][j+1] == 0:\n","                self.board[i-1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i-1 == 0 and self.board[i-1][j+1] == -1:\n","                    self.board[i-1][j+1] -= 1\n","                    self.score2 += 1  # Extra point for king promotion\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i-1][j+1]\n","                self.score1 -= abs(captured_piece)  # Remove from white's score\n","\n","                self.board[i-1][j+1] = 0\n","                self.board[i-2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                if i-2 == 0 and self.board[i-2][j+2] == -1:\n","                    self.board[i-2][j+2] -= 1\n","                    self.score2 += 1  # Extra point for king promotion\n","                act = self.Possible_cap(i-2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 2:  # backward-left\n","            if self.board[i+1][j-1] == 0:\n","                self.board[i+1][j-1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j-1]\n","                self.score1 -= abs(captured_piece)  # Remove from white's score\n","\n","                self.board[i+1][j-1] = 0\n","                self.board[i+2][j-2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i+2, j-2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","        elif move == 3:  # backward-right\n","            if self.board[i+1][j+1] == 0:\n","                self.board[i+1][j+1] = self.board[i][j]\n","                self.board[i][j] = 0\n","            else:\n","                # Capture opponent piece - update score\n","                captured_piece = self.board[i+1][j+1]\n","                self.score1 -= abs(captured_piece)  # Remove from white's score\n","\n","                self.board[i+1][j+1] = 0\n","                self.board[i+2][j+2] = self.board[i][j]\n","                self.board[i][j] = 0\n","                act = self.Possible_cap(i+2, j+2, player)\n","                if act:\n","                    for mv in act:\n","                        return self.apply_move(mv, player)\n","\n","\n","    def switch_player(self):\n","        self.current_player *= -1\n","\n","    def play(self):\n","        print(\"You are: HUMAN = -1 (Black, top, moves down)\")\n","        print(\"Agent is: +1 (White, bottom, moves up)\")\n","        print(\"Enter moves as: row col dir (dir=0 left, dir=1 right)\\n\")\n","\n","        while True:\n","            print(\"Current board:\")\n","            print(self.board)\n","            print(\"Current player:\", self.current_player)\n","\n","            legal = self.get_legal_moves(self.current_player)\n","            if not legal:\n","                print(\"No legal moves! Player\", self.current_player, \"loses.\")\n","                break\n","\n","            if self.current_player == -1:  # Human\n","                try:\n","                    move_in = input(\"Your move (i j dir): \")\n","                    i, j, d = map(int, move_in.split())\n","                    move = ((i, j), d)\n","                    if move not in legal:\n","                        print(\"Illegal move. Legal moves:\", legal[:7], \"...\")\n","                        continue\n","                    self.apply_move(move, -1)\n","                except Exception as e:\n","                    print(\"Bad input:\", e)\n","                    continue\n","            else:  # Agent\n","                move = random.choice(legal)\n","                print(\"Agent chooses:\", move)\n","                self.apply_move(move, +1)\n","\n","            self.switch_player()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":59431,"status":"error","timestamp":1758783263800,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"OQe7BBZtlPuM","outputId":"12883b0a-aa8d-4992-bba1-4b74b68313bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["You are: HUMAN = -1 (Black, top, moves down)\n","Agent is: +1 (White, bottom, moves up)\n","Enter moves as: row col dir (dir=0 left, dir=1 right)\n","\n","Current board:\n","[[ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]]\n","Current player: -1\n","Your move (i j dir): 2 1 0\n","Current board:\n","[[ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0  0  0 -1  0 -1  0 -1]\n"," [-1  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]]\n","Current player: 1\n","Agent chooses: ((5, 0), 1)\n","Current board:\n","[[ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0  0  0 -1  0 -1  0 -1]\n"," [-1  0  0  0  0  0  0  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]]\n","Current player: -1\n","Your move (i j dir): 2 3 0\n","Current board:\n","[[ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0  0  0  0  0 -1  0 -1]\n"," [-1  0 -1  0  0  0  0  0]\n"," [ 0  1  0  0  0  0  0  0]\n"," [ 0  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]]\n","Current player: 1\n","Agent chooses: ((5, 2), 1)\n","Current board:\n","[[ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0  0  0  0  0 -1  0 -1]\n"," [-1  0 -1  0  0  0  0  0]\n"," [ 0  1  0  1  0  0  0  0]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]]\n","Current player: -1\n","Your move (i j dir): 3 2 0\n","Illegal move. Legal moves: [((1, 0), 1), ((1, 2), 0), ((1, 2), 1), ((1, 4), 0), ((2, 5), 0), ((2, 5), 1), ((2, 7), 0)] ...\n","Current board:\n","[[ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0  0  0  0  0 -1  0 -1]\n"," [-1  0 -1  0  0  0  0  0]\n"," [ 0  1  0  1  0  0  0  0]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]]\n","Current player: -1\n"]},{"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-134948751.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-4139732775.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Human\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mmove_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your move (i j dir): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                     \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["game = Checkers()\n","game.play()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7tU2L3UNL7b8"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","\n","def play_against_agent(model, device, num_simulations=50):\n","    game = FixedCheckers()\n","    mcts = MCTS(model, device)\n","\n","    current_player = 1\n","\n","    while True:\n","        print(f\"Player {current_player}'s turn\")\n","        print(game.board)\n","\n","        # Check if game ended\n","        result = game.EndGame()\n","        if result != 0:\n","            if result == 1:\n","                print(\"White wins!\")\n","            else:\n","                print(\"Black wins!\")\n","            break\n","\n","        if current_player == 1:\n","            # Agent move\n","            policy, _ = mcts.search(game, num_simulations)\n","            # Mask invalid moves\n","            valid_moves_dict = game.get_valid_moves()\n","            valid_moves = game.get_moves(valid_moves_dict)\n","\n","            policy_mask = np.zeros_like(policy)\n","            valid_indices = []\n","            valid_probs = []\n","            for move in valid_moves:\n","                (i, j), direction = move\n","                idx = i * 32 + j * 4 + direction\n","                policy_mask[idx] = policy[idx]\n","                valid_indices.append(idx)\n","                valid_probs.append(policy[idx])\n","\n","            if len(valid_indices) == 0:\n","                print(\"No valid moves! Player loses.\")\n","                break\n","\n","            valid_probs = np.array(valid_probs)\n","            valid_probs = valid_probs / valid_probs.sum()\n","\n","            chosen_idx = np.random.choice(len(valid_indices), p=valid_probs)\n","            policy_idx = valid_indices[chosen_idx]\n","\n","            # Decode back to move\n","            i = policy_idx // 32\n","            j = (policy_idx % 32) // 4\n","            direction = policy_idx % 4\n","            action = ((i, j), direction)\n","            print(\"Agent chooses:\", action)\n","\n","            game.apply_move(action)\n","\n","        else:\n","            # Human input\n","            print(\"Enter your move as: i j direction (0-3)\")\n","            move_input = input()\n","            try:\n","                i, j, direction = map(int, move_input.strip().split())\n","                action = ((i, j), direction)\n","                # Validate move\n","                valid_moves_dict = game.get_valid_moves(current_player)\n","                valid_moves = game.get_moves(valid_moves_dict)\n","                if action not in valid_moves:\n","                    print(\"Invalid move! Try again.\")\n","                    continue\n","                game.apply_move(action)\n","            except:\n","                print(\"Invalid input! Try again.\")\n","                continue\n","\n","        # Switch player\n","        game.switch_player()\n","        current_player *= -1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":999},"executionInfo":{"elapsed":23314,"status":"error","timestamp":1758748453874,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"Xxmld52kitjw","outputId":"a20cc1c6-f6ef-4a21-8dba-e5c0fe4e56b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Player 1's turn\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  1  0  1  0  1]\n"," [ 0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Agent chooses: ((2, 3), 1)\n","Player -1's turn\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  0  0  1  0  1]\n"," [ 0  0  0  0  1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Enter your move as: i j direction (0-3)\n","5 0 1\n","Player 1's turn\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  0  0  1  0  1]\n"," [ 0  0  0  0  1  0  0  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Agent chooses: ((2, 7), 0)\n","Player -1's turn\n","[[ 0  1  0  1  0  1  0  1]\n"," [ 1  0  1  0  1  0  1  0]\n"," [ 0  1  0  0  0  1  0  0]\n"," [ 0  0  0  0  1  0  1  0]\n"," [ 0  0  0  0  0  0  0  0]\n"," [-1  0 -1  0 -1  0 -1  0]\n"," [ 0 -1  0 -1  0 -1  0 -1]\n"," [-1  0 -1  0 -1  0 -1  0]]\n","Enter your move as: i j direction (0-3)\n"]},{"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2192557108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_against_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1913920704.py\u001b[0m in \u001b[0;36mplay_against_agent\u001b[0;34m(model, device, num_simulations)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# Human input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your move as: i j direction (0-3)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mmove_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["play_against_agent(model, device, 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEXWlIDrijHr"},"outputs":[],"source":["model = models.Sequential()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1758652874545,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"_tLzh2_80dAP","outputId":"ae7dd81a-87b3-40a3-c6b1-4d0d4b4e9f36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Policy shape: torch.Size([1, 1024])\n","Value: -0.00334688788279891\n"]}],"source":["# Assuming you have a CheckersNet instance\n","model = CheckersNN( )\n","\n","# Forward pass\n","policy, value = model(board_tensor)\n","\n","print(\"Policy shape:\", policy.shape)  # should be [1, 1024]\n","print(\"Value:\", value.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-EVroWm_xyx"},"outputs":[],"source":["mask = torch.zeros_like(policy)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1758654853835,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"0dTt5zHh_yrQ","outputId":"f465389a-f791-4e43-9223-ee3fb646de3d"},"outputs":[{"data":{"text/plain":["torch.Size([1, 1024])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["mask.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"executionInfo":{"elapsed":36,"status":"error","timestamp":1758654818530,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"xJ4Jthna_fhq","outputId":"78b7f5e3-a19f-48ee-8f34-a04967121fc1"},"outputs":[{"ename":"IndexError","evalue":"index 33 is out of bounds for dimension 0 with size 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3251364351.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# legal move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: index 33 is out of bounds for dimension 0 with size 1"]}],"source":["available_moves=available(b,1)\n","mask = torch.zeros_like(policy)\n","for (i,j), moves in available_moves.items():\n","        for direction, valid in enumerate(moves):\n","            if valid:  # legal move\n","                index = (i*8 + j)*4 + direction\n","                mask[index] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbBD-qHs-Wc8"},"outputs":[],"source":["a=mask_illegal_moves(policy,available(b,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1758655041340,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"yQSYqhofAjSY","outputId":"f6d2ad56-4126-4a2e-de86-18953ebc494b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(40)\n"]}],"source":["max_index = torch.argmax(a)\n","print(max_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGcJxmX04UpP"},"outputs":[],"source":["import torch\n","\n","def mask_illegal_moves(policy, available_moves):\n","    # policy: tensor shape [1024]\n","    mask = torch.zeros_like(policy[0])  # all zeros\n","\n","    for (i,j), moves in available_moves.items():\n","        for direction, valid in enumerate(moves):\n","            if valid:  # legal move\n","                index = (i*8 + j)*4 + direction\n","                mask[index] = 1  # allow this move\n","\n","    # Apply mask: illegal moves become 0\n","    masked_policy = policy * mask\n","\n","    # Re-normalize to sum to 1\n","    if masked_policy.sum() > 0:\n","        masked_policy = masked_policy / masked_policy.sum()\n","\n","    return masked_policy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1758652018000,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"vtVYr8-M0h2m","outputId":"e2f4cd53-3c00-461a-e7bc-f7b99c19685c"},"outputs":[{"data":{"text/plain":["tensor([[0.0010, 0.0010, 0.0009,  ..., 0.0010, 0.0010, 0.0010]],\n","       grad_fn=<SoftmaxBackward0>)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BWbSqSPzx9R"},"outputs":[],"source":["game = CheckersNN()\n","game.board = b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNoOLIowysDi"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CheckersNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # Shared convolutional body\n","        self.conv1 = nn.Conv2d(6, 64, 3, padding=1)\n","        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n","\n","        # Policy head\n","        self.policy_conv = nn.Conv2d(128, 64, 1)\n","        self.policy_fc = nn.Linear(64*8*8, 1024)\n","\n","        # Value head\n","        self.value_conv = nn.Conv2d(128, 64, 1)\n","        self.value_fc1 = nn.Linear(64*8*8, 64)\n","        self.value_fc2 = nn.Linear(64, 1)\n","\n","    def forward(self, x):\n","        # Shared body\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","\n","        # Policy head\n","        p = F.relu(self.policy_conv(x))\n","        p = p.view(p.size(0), -1)\n","        p = self.policy_fc(p)\n","\n","        # Value head\n","        v = F.relu(self.value_conv(x))\n","        v = v.view(v.size(0), -1)\n","        v = F.relu(self.value_fc1(v))\n","        v = torch.tanh(self.value_fc2(v))\n","\n","        return p, v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb-rs1lZoDGl"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        # Two 3x3 convolutions inside residual block\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out = F.relu(x + out)  # Residual connection\n","        return out\n","\n","class CheckersCNN(nn.Module):\n","    def __init__(self, num_res_blocks=20, board_channels=6, torso_channels=256, policy_size=512):\n","        super().__init__()\n","        # Initial conv layer (like AlphaZero)\n","        self.conv_in = nn.Conv2d(board_channels, torso_channels, kernel_size=3, padding=1)\n","        self.bn_in = nn.BatchNorm2d(torso_channels)\n","\n","        # Residual blocks\n","        self.res_blocks = nn.ModuleList([ResidualBlock(torso_channels) for _ in range(num_res_blocks)])\n","\n","        # Policy head\n","        self.policy_conv = nn.Conv2d(torso_channels, 2, kernel_size=1)  # reduce channels to 2\n","        self.policy_bn = nn.BatchNorm2d(2)\n","        self.policy_fc = nn.Linear(2*8*8, policy_size)  # flatten 8x8x2 → FC\n","        # Policy output will be logits of size `policy_size`\n","\n","        # Value head\n","        self.value_conv = nn.Conv2d(torso_channels, 1, kernel_size=1)  # reduce channels to 1\n","        self.value_bn = nn.BatchNorm2d(1)\n","        self.value_fc1 = nn.Linear(8*8*1, 64)\n","        self.value_fc2 = nn.Linear(64, 1)  # tanh output [-1,1]\n","\n","    def forward(self, x):\n","        # Input conv\n","        x = F.relu(self.bn_in(self.conv_in(x)))\n","\n","        # Residual torso\n","        for block in self.res_blocks:\n","            x = block(x)\n","\n","        # Policy head\n","        p = F.relu(self.policy_bn(self.policy_conv(x)))\n","        p = p.view(p.size(0), -1)\n","        p = self.policy_fc(p)  # raw logits\n","\n","        # Value head\n","        v = F.relu(self.value_bn(self.value_conv(x)))\n","        v = v.view(v.size(0), -1)\n","        v = F.relu(self.value_fc1(v))\n","        v = torch.tanh(self.value_fc2(v))  # output between -1 and 1\n","\n","        return p, v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAZKK2lBFX7g"},"outputs":[],"source":["\n","\n","import torch\n","\n","def ucb_scores(Q, N, P, c):\n","    \"\"\"\n","    Compute U(s,a) for all actions from a state.\n","\n","    Q: tensor [num_actions] - action value estimates\n","    N: tensor [num_actions] - visit counts for each action\n","    P: tensor [num_actions] - NN priors for each action\n","    c: float - exploration constant\n","    \"\"\"\n","    total_N = torch.sum(N)  # ∑b N(s,b)\n","    u = Q + c * P * (torch.sqrt(total_N + 1e-8) / (1 + N))\n","    return u"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90,"status":"ok","timestamp":1758658043517,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"MuvbWiWEFX4-","outputId":"d9414481-c6e7-417c-96d8-f7556c29e31e"},"outputs":[{"name":"stdout","output_type":"stream","text":["U(s,a): tensor([0.4287, 0.5811, 0.4193, 2.4156])\n","Chosen action index: 3\n"]}],"source":["# Example with 4 actions\n","Q = torch.tensor([0.2, 0.5, 0.0, -0.1], dtype=torch.float32)\n","N = torch.tensor([10, 30, 5, 0], dtype=torch.float32)\n","P = torch.tensor([0.25, 0.25, 0.25, 0.25], dtype=torch.float32)  # uniform prior\n","c = 1.5\n","\n","U = ucb_scores(Q, N, P, c)\n","print(\"U(s,a):\", U)\n","\n","# Pick best action\n","best_action = torch.argmax(U).item()\n","print(\"Chosen action index:\", best_action)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iidup3cpFX2W"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","\n","class Node:\n","    def __init__(self, state, player, prior=0.0, parent=None):\n","        self.state = state            # game state (board)\n","        self.player = player          # current player\n","        self.prior = prior            # P(s,a) from NN\n","        self.parent = parent\n","        self.children = {}            # action → Node\n","\n","        self.N = 0                    # visit count\n","        self.W = 0.0                  # total value\n","        self.Q = 0.0                  # mean value\n","\n","    def is_expanded(self):\n","        return len(self.children) > 0\n","\n","\n","def ucb_score(parent, child, c=1.5):\n","    \"\"\"\n","    U(s,a) = Q(s,a) + c * P(s,a) * sqrt(sum_b N(s,b)) / (1 + N(s,a))\n","    \"\"\"\n","    Q = child.Q\n","    Nsa = child.N\n","    Psa = child.prior\n","    total_N = sum(ch.N for ch in parent.children.values())\n","    return Q + c * Psa * ( (total_N**0.5) / (1 + Nsa) )\n","\n","\n","def select_child(node, c=1.5):\n","    \"\"\"Select action with highest U(s,a).\"\"\"\n","    return max(node.children.items(), key=lambda item: ucb_score(node, item[1], c))\n","\n","\n","def expand(node, model, game):\n","    \"\"\"\n","    Expand node with NN priors.\n","    \"\"\"\n","    # Convert board to tensor (5×8×8 channels as input)\n","    board_tensor = game.state_to_tensor(node.state).unsqueeze(0)\n","    policy, value = model(board_tensor)\n","    policy = F.softmax(policy, dim=1)[0]  # [1024]\n","\n","    # Mask illegal moves\n","    available_moves = game.available(node.player)\n","    policy = mask_illegal_moves(policy, available_moves)\n","\n","    # Create child nodes for each legal move\n","    for (i,j), moves in available_moves.items():\n","        for direction, valid in enumerate(moves):\n","            if valid:\n","                idx = (i*8 + j)*4 + direction\n","                next_state = game.apply_move(node.state, (i,j,direction), node.player)\n","                node.children[(i,j,direction)] = Node(next_state, -node.player, prior=policy[idx], parent=node)\n","\n","    return value.item()\n","\n","\n","def backpropagate(path, value):\n","    \"\"\"\n","    Backup value along the path.\n","    Flip sign when changing player.\n","    \"\"\"\n","    for node in reversed(path):\n","        node.N += 1\n","        node.W += value\n","        node.Q = node.W / node.N\n","        value = -value  # opponent’s perspective\n","\n","\n","def run_mcts(root, model, game, simulations=50, c=1.5):\n","    \"\"\"\n","    Run MCTS from root node.\n","    \"\"\"\n","    for _ in range(simulations):\n","        node = root\n","        path = [node]\n","\n","        # 1. Selection\n","        while node.is_expanded():\n","            action, node = select_child(node, c)\n","            path.append(node)\n","\n","        # 2. Expansion + Evaluation\n","        value = expand(node, model, game)\n","\n","        # 3. Backpropagation\n","        backpropagate(path, value)\n","\n","    # Return action with highest visit count\n","    best_action = max(root.children.items(), key=lambda item: item[1].N)[0]\n","    return best_action\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYXMgM_CFXzu"},"outputs":[],"source":["import numpy as np\n","import torch\n","\n","class Checkers:\n","    def __init__(self):\n","        self.board = np.array([\n","            [0, 1, 0, 1, 0, 1, 0, 1],\n","            [1, 0, 1, 0, 1, 0, 1, 0],\n","            [0, 0, 0, 1, 0, 1, 0, 1],\n","            [1, 0, 0, 0, 0, 0, 0, 0],\n","            [0, 0, 0, 0, 0, 0, 0, 0],\n","            [-1, 0, -1, 0, -1, 0, -1, 0],\n","            [0, -1, 0, -1, 0, -1, 0, -1],\n","            [-1, 0, -1, 0, -1, 0, -1, 0]\n","        ], dtype=int)\n","\n","        # Convert board to tensor\n","        self.board_tensor = self.board_to_tensor(self.board)\n","    def available(self, player):\n","        d = defaultdict(lambda: np.zeros(4))\n","\n","        if player == -1:  # black pieces\n","            for i in range(len(self.board)):\n","                for j in range(len(self.board)):\n","                    if self.board[i][j] != 0 and self.board[i][j] <= player:\n","                        # Forward left\n","                        if i-1 >= 0 and j-1 >= 0:\n","                            if self.board[i-1][j-1] == 0:\n","                                d[i, j][0] = 1\n","                            elif self.board[i-1][j-1] >= 1:\n","                                if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                                    d[i, j][0] = 1\n","                        # Forward right\n","                        if i-1 >= 0 and j+1 < 8:\n","                            if self.board[i-1][j+1] == 0:\n","                                d[i, j][1] = 1\n","                            elif self.board[i-1][j+1] >= 1:\n","                                if i-2 >= 0 and j+2 < 8 and self.board[i-2][j+2] == 0:\n","                                    d[i, j][1] = 1\n","\n","                    if self.board[i][j] != 0 and self.board[i][j] < player:\n","                        # Backward left\n","                        if i+1 < 8 and j-1 >= 0:\n","                            if self.board[i+1][j-1] == 0:\n","                                d[i, j][2] = 1\n","                            elif self.board[i+1][j-1] >= 1:\n","                                if i+2 < 8 and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                                    d[i, j][2] = 1\n","                        # Backward right\n","                        if i+1 < 8 and j+1 < 8:\n","                            if self.board[i+1][j+1] == 0:\n","                                d[i, j][3] = 1\n","                            elif self.board[i+1][j+1] >= 1:\n","                                if i+2 < 8 and j+2 < 8 and self.board[i+2][j+2] == 0:\n","                                    d[i, j][3] = 1\n","\n","        if player == 1:  # white pieces\n","            for i in range(len(self.board)):\n","                for j in range(len(self.board)):\n","                    if self.board[i][j] != 0 and self.board[i][j] >= player:\n","                        # Forward left\n","                        if i+1 < 8 and j-1 >= 0:\n","                            if self.board[i+1][j-1] == 0:\n","                                d[i, j][0] = 1\n","                            elif self.board[i+1][j-1] <= 0:\n","                                if i+2 < 8 and j-2 >= 0 and self.board[i+2][j-2] == 0:\n","                                    d[i, j][0] = 1\n","                        # Forward right\n","                        if i+1 < 8 and j+1 < 8:\n","                            if self.board[i+1][j+1] == 0:\n","                                d[i, j][1] = 1\n","                            elif self.board[i+1][j+1] <= 0:\n","                                if i+2 < 8 and j+2 < 8 and self.board[i+2][j+2] == 0:\n","                                    d[i, j][1] = 1\n","\n","                    if self.board[i][j] != 0 and self.board[i][j] > player:\n","                        # Backward left\n","                        if i-1 >= 0 and j-1 >= 0:\n","                            if self.board[i-1][j-1] == 0:\n","                                d[i, j][2] = 1\n","                            elif self.board[i-1][j-1] <= 0:\n","                                if i-2 >= 0 and j-2 >= 0 and self.board[i-2][j-2] == 0:\n","                                    d[i, j][2] = 1\n","                        # Backward right\n","                        if i-1 >= 0 and j+1 < 8:\n","                            if self.board[i-1][j+1] == 0:\n","                                d[i, j][3] = 1\n","                            elif self.board[i-1][j+1] <= 0:\n","                                if i-2 >= 0 and j+2 < 8 and self.board[i-2][j+2] == 0:\n","                                    d[i, j][3] = 1\n","        return d\n","\n","    def get_start_state(self):\n","        return self.board.copy()\n","\n","    def is_game_over(self, state):\n","        # simple check: no moves for either player\n","        return len(self.get_valid_moves(state, 1)) == 0 or len(self.get_valid_moves(state, -1)) == 0\n","\n","    def winner(self, state):\n","        if len(self.get_valid_moves(state, 1)) == 0:\n","            return -1\n","        elif len(self.get_valid_moves(state, -1)) == 0:\n","            return 1\n","        return 0\n","\n","\n","\n","    def get_valid_moves(self, state, player):\n","        d = self.available(state, player)\n","        moves = []\n","        for (i,j), dirs in d.items():\n","            for dir_idx, flag in enumerate(dirs):\n","                if flag == 1:\n","                    moves.append((i, j, dir_idx))  # move = (row, col, direction)\n","        return moves\n","\n","    def apply_move(self, state, move, player):\n","        \"\"\"Return new state after applying move\"\"\"\n","        i, j, d = move\n","        new_state = state.copy()\n","        # apply direction like your available() (0=FL,1=FR,2=BL,3=BR)\n","        if d == 0: new_state[i][j], new_state[i-1][j-1] = 0, state[i][j]\n","        if d == 1: new_state[i][j], new_state[i-1][j+1] = 0, state[i][j]\n","        if d == 2: new_state[i][j], new_state[i+1][j-1] = 0, state[i][j]\n","        if d == 3: new_state[i][j], new_state[i+1][j+1] = 0, state[i][j]\n","        return new_state\n","\n","    def move_to_index(self, move):\n","        i, j, d = move\n","        return (i * 8 + j) * 4 + d   # 0–1023\n","\n","    def state_to_tensor(self, state):\n","        # channels: [white, black, empty, current turn white, current turn black]\n","        white = (state == 1).astype(np.float32)\n","        black = (state == -1).astype(np.float32)\n","        empty = (state == 0).astype(np.float32)\n","        # here: 2 dummy channels for player perspective\n","        return torch.tensor(np.stack([white, black, empty, np.zeros((8,8)), np.zeros((8,8))]), dtype=torch.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DxNKULXFXw3"},"outputs":[],"source":["zzimport math\n","import torch.nn.functional as F\n","\n","class Node:\n","    def __init__(self, state, player, parent=None):\n","        self.state = state\n","        self.player = player\n","        self.parent = parent\n","        self.children = {}\n","        self.N = 0\n","        self.W = 0\n","        self.Q = 0\n","        self.P = 0\n","\n","def run_mcts(root, model, game, simulations=50, c_puct=1.0):\n","    for _ in range(simulations):\n","        node = root\n","        path = [node]\n","\n","        # 1. Selection\n","        while node.children:\n","            total_N = sum(child.N for child in node.children.values())\n","            best_score, best_child = -float('inf'), None\n","            for action, child in node.children.items():\n","                U = child.Q + c_puct * child.P * math.sqrt(total_N) / (1 + child.N)\n","                if U > best_score:\n","                    best_score, best_child = U, child\n","            node = best_child\n","            path.append(node)\n","\n","        # 2. Expansion\n","        state_tensor = game.state_to_tensor(node.state).unsqueeze(0)\n","        policy_logits, value = model(state_tensor)\n","        policy = F.softmax(policy_logits, dim=1).detach().cpu().numpy()[0]\n","\n","        valid_moves = game.get_valid_moves(node.state, node.player)\n","        for move in valid_moves:\n","            idx = game.move_to_index(move)\n","            child_state = game.apply_move(node.state, move, node.player)\n","            child = Node(child_state, -node.player, parent=node)\n","            child.P = policy[idx]\n","            node.children[move] = child\n","\n","        leaf_value = value.item() if not game.is_game_over(node.state) else game.winner(node.state)\n","\n","        # 3. Backprop\n","        for n in path:\n","            n.N += 1\n","            n.W += leaf_value if n.player == root.player else -leaf_value\n","            n.Q = n.W / n.N\n","\n","    return max(root.children.items(), key=lambda x: x[1].N)[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAcI43e1FXqB"},"outputs":[],"source":["def self_play_episode(model, game, simulations=50):\n","    states, policies, players = [], [], []\n","    state = game.get_start_state()\n","    player = 1\n","\n","    while not game.is_game_over(state):\n","        root = Node(state, player)\n","        best_move = run_mcts(root, model, game, simulations)\n","        pi = get_policy_target(root, game)\n","\n","        states.append(game.state_to_tensor(state))\n","        policies.append(pi)\n","        players.append(player)\n","\n","        state = game.apply_move(state, best_move, player)\n","        player = -player\n","\n","    z = game.winner(state)\n","    data = []\n","    for s, pi, p in zip(states, policies, players):\n","        data.append((s, pi, z * p))\n","    return data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsH6gxd5PmkL"},"outputs":[],"source":["import math\n","import torch.nn.functional as F\n","\n","class Node:\n","    def __init__(self, state, player, parent=None):\n","        self.state = state\n","        self.player = player\n","        self.parent = parent\n","        self.children = {}\n","        self.N = 0\n","        self.W = 0\n","        self.Q = 0\n","        self.P = 0\n","\n","def run_mcts(root, model, game, simulations=50, c_puct=1.0):\n","    for _ in range(simulations):\n","        node = root\n","        path = [node]\n","\n","        # 1. Selection\n","        while node.children:\n","            total_N = sum(child.N for child in node.children.values())\n","            best_score, best_child = -float('inf'), None\n","            for action, child in node.children.items():\n","                U = child.Q + c_puct * child.P * math.sqrt(total_N) / (1 + child.N)\n","                if U > best_score:\n","                    best_score, best_child = U, child\n","            node = best_child\n","            path.append(node)\n","\n","        # 2. Expansion\n","        state_tensor = game.state_to_tensor(node.state).unsqueeze(0)\n","        policy_logits, value = model(state_tensor)\n","        policy = F.softmax(policy_logits, dim=1).detach().cpu().numpy()[0]\n","\n","        valid_moves = game.get_valid_moves(node.state, node.player)\n","        for move in valid_moves:\n","            idx = game.move_to_index(move)\n","            child_state = game.apply_move(node.state, move, node.player)\n","            child = Node(child_state, -node.player, parent=node)\n","            child.P = policy[idx]\n","            node.children[move] = child\n","\n","        leaf_value = value.item() if not game.is_game_over(node.state) else game.winner(node.state)\n","\n","        # 3. Backprop\n","        for n in path:\n","            n.N += 1\n","            n.W += leaf_value if n.player == root.player else -leaf_value\n","            n.Q = n.W / n.N\n","\n","    return max(root.children.items(), key=lambda x: x[1].N)[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F3w3zyogFXng"},"outputs":[],"source":["def train(model, optimizer, data, batch_size=64, epochs=1):\n","    model.train()\n","    states, policies, values = zip(*data)\n","    states = torch.stack(states)\n","    policies = torch.stack(policies)\n","    values = torch.tensor(values, dtype=torch.float32).unsqueeze(1)\n","\n","    dataset = torch.utils.data.TensorDataset(states, policies, values)\n","    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(epochs):\n","        for s, pi, z in loader:\n","            optimizer.zero_grad()\n","            p_pred, v_pred = model(s)\n","            policy_loss = -torch.sum(pi * F.log_softmax(p_pred, dim=1)) / s.size(0)\n","            value_loss = F.mse_loss(v_pred, z)\n","            loss = policy_loss + value_loss\n","            loss.backward()\n","            optimizer.step()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1758652792295,"user":{"displayName":"soul less","userId":"12195366273789735218"},"user_tz":-330},"id":"5DV2m0u9zvrQ","outputId":"756a90f4-80cf-48db-adb8-8f3664cad471"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 5, 8, 8])\n"]}],"source":["import torch\n","import numpy as np\n","\n","# Your board as numpy array\n","board_np = np.array([\n","    [0, 1, 0, 1, 0, 1, 0, 1],\n","    [1, 0, 1, 0, 1, 0, 1, 0],\n","    [0, 0, 0, 1, 0, 1, 0, 1],\n","    [1, 0, 0, 0, 0, 0, 0, 0],\n","    [0, 0, 0, 0, 0, 0, 0, 0],\n","    [-1, 0, -1, 0, -1, 0, -1, 0],\n","    [0, -1, 0, -1, 0, -1, 0, -1],\n","    [-1, 0, -1, 0, -1, 0, -1, 0]\n","])\n","\n","# Create channels\n","white_channel = (board_np == 1).astype(np.float32)\n","black_channel = (board_np == -1).astype(np.float32)\n","king_w =   (board_np == 2).astype(np.float32)\n","king_b=(board_np == -2).astype(np.float32)\n","empty=(board_np == 0).astype(np.float32)\n","\n","# Stack into tensor: shape (channels, 8, 8)\n","board_tensor = np.stack([white_channel, black_channel, king_w,king_b, empty], axis=0)\n","\n","# Add batch dimension: shape (1, channels, 8, 8)\n","board_tensor = torch.tensor(board_tensor).unsqueeze(0)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNowdiw6fkvqcIpHYV2Ldhz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}